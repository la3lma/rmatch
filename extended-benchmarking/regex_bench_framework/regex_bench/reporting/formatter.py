"""
Report formatters for different output formats.
"""

import json
import time
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

# Chart generation libraries
try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False


class HTMLFormatter:
    """Generate HTML reports with interactive features."""

    def generate_report(
        self,
        data: Dict[str, Any],
        output_dir: Path,
        include_charts: bool = False
    ) -> Path:
        """Generate an HTML report."""

        # Generate report content
        html_content = self._generate_html_report(data, include_charts)

        # Write report file
        report_file = output_dir / "benchmark_report.html"
        with open(report_file, 'w') as f:
            f.write(html_content)

        # Copy any assets if needed
        self._copy_assets(output_dir)

        return report_file

    def _generate_html_report(self, data: Dict[str, Any], include_charts: bool) -> str:
        """Generate the complete HTML report content."""

        summary = data.get('summary', {})
        raw_results = data.get('raw_results', [])
        analysis = data.get('analysis', {})
        metadata = data.get('metadata', {})

        html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Regex Benchmark Report - {summary.get('run_id', 'Unknown')}</title>
    <style>
        {self._get_css_styles()}
    </style>
    {self._get_chart_scripts() if include_charts else ''}
</head>
<body>
    <div class="container">
        <header>
            <h1>ğŸš€ Regex Benchmark Report</h1>
            <div class="metadata">
                <span class="run-id">Run ID: {summary.get('run_id', 'Unknown')}</span>
                <span class="timestamp">Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</span>
            </div>
        </header>

        {self._generate_summary_section(summary, analysis)}
        {self._generate_engines_section(raw_results)}
        {self._generate_results_section(raw_results, analysis)}
        {self._generate_performance_section(analysis)}
        {'    ' + self._generate_charts_section(raw_results, analysis) if include_charts else ''}
        {self._generate_raw_data_section(raw_results)}

        <footer>
            <p>Generated by Regex Bench Framework v2.0 â€¢ <a href="https://github.com/anthropics/claude-code">Powered by Claude Code</a></p>
        </footer>
    </div>
</body>
</html>"""

        return html

    def _generate_summary_section(self, summary: Dict[str, Any], analysis: Dict[str, Any] = None) -> str:
        """Generate the summary section with throughput statistics."""
        phase = str(summary.get('phase', 'unknown'))
        status = str(summary.get('status', 'unknown'))
        engines_tested = summary.get('engines_tested', [])
        engines = ', '.join([str(e) for e in engines_tested]) if engines_tested else 'None'
        total = int(summary.get('total_combinations', 0))
        successful = int(summary.get('successful_runs', 0))
        failed = int(summary.get('failed_runs', 0))
        duration = float(summary.get('duration_seconds', 0))

        status_class = 'status-success' if status.lower() == 'completed' else 'status-error'

        # Build main summary cards
        summary_cards = f"""
                <div class="summary-card">
                    <div class="card-label">Phase</div>
                    <div class="card-value">{phase.title()}</div>
                </div>
                <div class="summary-card">
                    <div class="card-label">Status</div>
                    <div class="card-value {status_class}">{status.title()}</div>
                </div>
                <div class="summary-card">
                    <div class="card-label">Duration</div>
                    <div class="card-value">{duration:.2f}s</div>
                </div>
                <div class="summary-card">
                    <div class="card-label">Engines Tested</div>
                    <div class="card-value">{engines}</div>
                </div>
                <div class="summary-card">
                    <div class="card-label">Total Runs</div>
                    <div class="card-value">{total}</div>
                </div>
                <div class="summary-card">
                    <div class="card-label">Success Rate</div>
                    <div class="card-value">{(successful/max(total,1)*100):.1f}%</div>
                </div>"""

        # Add throughput statistics if available
        throughput_section = ""
        if analysis and 'summary' in analysis and 'throughput_summary' in analysis['summary']:
            throughput_data = analysis['summary']['throughput_summary']
            if throughput_data:
                throughput_section = """
            <div class="throughput-summary">
                <h3>ğŸš€ Throughput Performance Summary</h3>
                <div class="throughput-grid">"""

                for engine_name, stats in throughput_data.items():
                    throughput_section += f"""
                    <div class="throughput-card">
                        <h4>{engine_name}</h4>
                        <div class="throughput-stats">
                            <div class="stat-item">
                                <span class="stat-label">Average:</span>
                                <span class="stat-value">{stats['average_mean_mbps']:.2f} MB/s</span>
                            </div>
                            <div class="stat-item">
                                <span class="stat-label">Median:</span>
                                <span class="stat-value">{stats['average_median_mbps']:.2f} MB/s</span>
                            </div>
                            <div class="stat-item">
                                <span class="stat-label">Std Dev:</span>
                                <span class="stat-value">{stats['average_std_dev_mbps']:.2f} MB/s</span>
                            </div>
                            <div class="stat-item">
                                <span class="stat-label">Peak:</span>
                                <span class="stat-value">{stats['best_mean_mbps']:.2f} MB/s</span>
                            </div>
                        </div>
                    </div>"""

                throughput_section += """
                </div>
            </div>"""

        return f"""
        <section class="summary">
            <h2>ğŸ“Š Benchmark Summary</h2>
            <div class="summary-grid">
                {summary_cards}
            </div>
            {throughput_section}
        </section>"""

    def _generate_engines_section(self, raw_results: List[Dict[str, Any]]) -> str:
        """Generate the engines overview section."""
        # Group results by engine
        engines = {}
        for result in raw_results:
            # Skip None results (incomplete jobs)
            if result is None:
                continue

            engine_name = result.get('engine_name', 'unknown')
            if engine_name not in engines:
                engines[engine_name] = {
                    'name': engine_name,
                    'runs': 0,
                    'successful_runs': 0,
                    'avg_compilation_ns': 0,
                    'avg_scanning_ns': 0,
                    'total_matches': 0
                }

            engines[engine_name]['runs'] += 1
            if result.get('status') == 'ok':
                engines[engine_name]['successful_runs'] += 1
                # Handle None values that might come from incomplete jobs
                compilation_ns = result.get('compilation_ns', 0) or 0
                scanning_ns = result.get('scanning_ns', 0) or 0
                match_count = result.get('match_count', 0) or 0
                engines[engine_name]['avg_compilation_ns'] += compilation_ns
                engines[engine_name]['avg_scanning_ns'] += scanning_ns
                engines[engine_name]['total_matches'] += match_count

        # Calculate averages
        for engine in engines.values():
            if engine['successful_runs'] > 0:
                engine['avg_compilation_ns'] //= engine['successful_runs']
                engine['avg_scanning_ns'] //= engine['successful_runs']

        engine_rows = ""
        for engine in engines.values():
            success_rate = (engine['successful_runs'] / max(engine['runs'], 1)) * 100
            engine_rows += f"""
                <tr>
                    <td class="engine-name">{engine['name']}</td>
                    <td>{engine['runs']}</td>
                    <td>{success_rate:.1f}%</td>
                    <td>{engine['avg_compilation_ns']:,} ns</td>
                    <td>{engine['avg_scanning_ns']:,} ns</td>
                    <td>{engine['total_matches']}</td>
                </tr>"""

        return f"""
        <section class="engines">
            <h2>ğŸ”§ Engine Performance Overview</h2>
            <div class="table-container">
                <table class="results-table">
                    <thead>
                        <tr>
                            <th>Engine</th>
                            <th>Total Runs</th>
                            <th>Success Rate</th>
                            <th>Avg Compilation</th>
                            <th>Avg Scanning</th>
                            <th>Total Matches</th>
                        </tr>
                    </thead>
                    <tbody>
                        {engine_rows}
                    </tbody>
                </table>
            </div>
        </section>"""

    def _generate_results_section(self, raw_results: List[Dict[str, Any]], analysis: Dict[str, Any]) -> str:
        """Generate detailed results section."""
        if not raw_results:
            return "<section><h2>ğŸ“‹ Results</h2><p>No results available.</p></section>"

        result_rows = ""
        for i, result in enumerate(raw_results):
            # Skip None results (incomplete jobs)
            if result is None:
                continue

            status_class = 'status-success' if result.get('status') == 'ok' else 'status-error'
            compilation_ms = ((result.get('compilation_ns') or 0) / 1_000_000)
            scanning_ms = ((result.get('scanning_ns') or 0) / 1_000_000)

            # Calculate corpus size and MB/sec throughput
            corpus_size_bytes = result.get('corpus_size_bytes') or 0
            corpus_size_mb = corpus_size_bytes / (1024 * 1024) if corpus_size_bytes and corpus_size_bytes > 0 else 0

            # Calculate MB/sec scanning throughput
            scanning_ns = result.get('scanning_ns') or 0
            mb_per_sec = 0
            if scanning_ns and scanning_ns > 0 and corpus_size_bytes and corpus_size_bytes > 0:
                scanning_seconds = scanning_ns / 1_000_000_000  # Convert nanoseconds to seconds
                mb_per_sec = corpus_size_mb / scanning_seconds

            # Calculate pattern processing rate (regex/sec)
            patterns_per_sec_display = "-"
            patterns_compiled = result.get('patterns_compiled', 0)
            if scanning_ns and scanning_ns > 0 and patterns_compiled and patterns_compiled > 0:
                scanning_seconds = scanning_ns / 1_000_000_000  # Convert nanoseconds to seconds
                patterns_per_sec = patterns_compiled / scanning_seconds
                if patterns_per_sec >= 1000:
                    patterns_per_sec_display = f"{patterns_per_sec/1000:.1f}K/sec"
                else:
                    patterns_per_sec_display = f"{patterns_per_sec:.1f}/sec"

            # Calculate total pattern throughput (patterns Ã— MB/sec)
            total_throughput_display = "-"
            if scanning_ns and scanning_ns > 0 and patterns_compiled and patterns_compiled > 0 and corpus_size_mb > 0:
                scanning_seconds = scanning_ns / 1_000_000_000  # Convert nanoseconds to seconds
                total_throughput = (patterns_compiled * corpus_size_mb) / scanning_seconds
                if total_throughput >= 1000:
                    total_throughput_display = f"{total_throughput/1000:.1f}K pâ‹…MB/s"
                else:
                    total_throughput_display = f"{total_throughput:.1f} pâ‹…MB/s"

            result_rows += f"""
                <tr class="result-row">
                    <td>{i + 1}</td>
                    <td>{result.get('engine_name', 'Unknown')}</td>
                    <td>{result.get('iteration', 0)}</td>
                    <td class="{status_class}">{result.get('status', 'unknown')}</td>
                    <td>{corpus_size_mb:.1f} MB</td>
                    <td>{compilation_ms:.3f}</td>
                    <td>{scanning_ms:.3f}</td>
                    <td>{mb_per_sec:.2f} MB/sec</td>
                    <td>{result.get('match_count', 0)}</td>
                    <td>{result.get('patterns_compiled', 0)}</td>
                    <td>{patterns_per_sec_display}</td>
                    <td>{total_throughput_display}</td>
                </tr>"""

        return f"""
        <section class="detailed-results">
            <h2>ğŸ“‹ Detailed Results</h2>
            <div class="table-container">
                <table class="results-table">
                    <thead>
                        <tr>
                            <th>#</th>
                            <th>Engine</th>
                            <th>Iteration</th>
                            <th>Status</th>
                            <th>Corpus Size</th>
                            <th>Compilation (ms)</th>
                            <th>Scanning (ms)</th>
                            <th>Throughput</th>
                            <th>Matches</th>
                            <th>Patterns</th>
                            <th>Regex/sec</th>
                            <th>Total Throughput</th>
                        </tr>
                    </thead>
                    <tbody>
                        {result_rows}
                    </tbody>
                </table>
            </div>
        </section>"""

    def _generate_performance_section(self, analysis: Dict[str, Any]) -> str:
        """Generate performance analysis section."""
        grouped_stats = analysis.get('grouped_statistics', {})
        comparisons = analysis.get('comparisons', {})

        if not grouped_stats:
            return "<section><h2>âš¡ Performance Analysis</h2><p>No analysis data available.</p></section>"

        stats_content = ""
        for group_key, stats in grouped_stats.items():
            engine_name = stats.get('engine_name', 'Unknown')
            patterns = stats.get('patterns_compiled', 0)
            corpus_size = stats.get('corpus_size_bytes', 0)

            # Get scanning statistics
            scanning = stats.get('scanning', {})
            if scanning:
                mean_ms = (scanning.get('mean') or 0) / 1_000_000
                median_ms = (scanning.get('median') or 0) / 1_000_000
                std_ms = (scanning.get('std_dev') or 0) / 1_000_000

                stats_content += f"""
                <div class="performance-card">
                    <h4>{engine_name}</h4>
                    <div class="perf-details">
                        <div class="perf-item">
                            <span class="label">Patterns:</span>
                            <span class="value">{patterns}</span>
                        </div>
                        <div class="perf-item">
                            <span class="label">Corpus Size:</span>
                            <span class="value">{corpus_size:,} bytes</span>
                        </div>
                        <div class="perf-item">
                            <span class="label">Mean Scan Time:</span>
                            <span class="value">{mean_ms:.3f} ms</span>
                        </div>
                        <div class="perf-item">
                            <span class="label">Median Scan Time:</span>
                            <span class="value">{median_ms:.3f} ms</span>
                        </div>
                        <div class="perf-item">
                            <span class="label">Std Deviation:</span>
                            <span class="value">{std_ms:.3f} ms</span>
                        </div>
                    </div>
                </div>"""

        return f"""
        <section class="performance">
            <h2>âš¡ Performance Analysis</h2>
            <div class="performance-grid">
                {stats_content}
            </div>
        </section>"""

    def _generate_charts_section(self, raw_results: List[Dict[str, Any]], analysis: Dict[str, Any]) -> str:
        """Generate interactive charts section using Plotly."""
        if not PLOTLY_AVAILABLE:
            return """
        <section class="charts">
            <h2>ğŸ“ˆ Performance Charts</h2>
            <div class="chart-placeholder">
                <p>ğŸ“Š Interactive charts require plotly library. Install with: pip install plotly</p>
                <p>Charts would include: throughput comparisons, compilation time distributions, and performance trends.</p>
            </div>
        </section>"""

        # Skip chart generation if insufficient data
        if not raw_results or len(raw_results) < 2:
            return """
        <section class="charts">
            <h2>ğŸ“ˆ Performance Charts</h2>
            <div class="chart-placeholder">
                <p>ğŸ“Š Insufficient data for charts. Need at least 2 completed results.</p>
            </div>
        </section>"""

        try:
            # Generate interactive charts
            throughput_chart = self._create_throughput_comparison_chart(raw_results)
            timing_chart = self._create_timing_distribution_chart(raw_results)
            performance_chart = self._create_engine_performance_chart(raw_results, analysis)

            return f"""
        <section class="charts">
            <h2>ğŸ“ˆ Interactive Performance Charts</h2>
            <div class="chart-grid">
                <div class="chart-container">
                    <h3>ğŸš€ Engine Throughput Comparison</h3>
                    {throughput_chart}
                </div>
                <div class="chart-container">
                    <h3>â±ï¸ Timing Distribution</h3>
                    {timing_chart}
                </div>
                <div class="chart-container">
                    <h3>ğŸ“Š Overall Performance Analysis</h3>
                    {performance_chart}
                </div>
            </div>
        </section>"""
        except Exception as e:
            return f"""
        <section class="charts">
            <h2>ğŸ“ˆ Performance Charts</h2>
            <div class="chart-error">
                <p>âŒ Error generating charts: {str(e)}</p>
                <p>Please check your data and try again.</p>
            </div>
        </section>"""

    def _create_throughput_comparison_chart(self, raw_results: List[Dict[str, Any]]) -> str:
        """Create interactive throughput comparison chart."""
        if not PLOTLY_AVAILABLE:
            return "<p>Plotly not available</p>"

        # Extract completed results with valid data
        valid_results = []
        for result in raw_results:
            if (result and result.get('status') == 'ok' and
                result.get('scanning_ns') and result.get('corpus_size_bytes')):
                # Calculate throughput in MB/s
                scanning_s = result['scanning_ns'] / 1_000_000_000
                corpus_mb = result['corpus_size_bytes'] / (1024 * 1024)
                if scanning_s > 0:
                    throughput = corpus_mb / scanning_s
                    valid_results.append({
                        'engine': result.get('engine_name', 'Unknown'),
                        'throughput': throughput,
                        'patterns': result.get('pattern_count', 0),
                        'corpus_mb': corpus_mb
                    })

        if len(valid_results) < 2:
            return "<p>Not enough valid results for throughput chart</p>"

        # Group by engine and calculate average throughput
        engines = {}
        for result in valid_results:
            engine = result['engine']
            if engine not in engines:
                engines[engine] = []
            engines[engine].append(result['throughput'])

        engine_names = list(engines.keys())
        avg_throughput = [sum(engines[engine])/len(engines[engine]) for engine in engine_names]
        max_throughput = [max(engines[engine]) for engine in engine_names]

        # Create bar chart
        fig = go.Figure(data=[
            go.Bar(name='Average Throughput', x=engine_names, y=avg_throughput,
                   marker_color='#3498db', opacity=0.8),
            go.Bar(name='Peak Throughput', x=engine_names, y=max_throughput,
                   marker_color='#2ecc71', opacity=0.6)
        ])

        fig.update_layout(
            title="Engine Throughput Comparison (MB/s)",
            xaxis_title="Engine",
            yaxis_title="Throughput (MB/s)",
            barmode='group',
            template="plotly_white",
            height=400,
            showlegend=True
        )

        return fig.to_html(include_plotlyjs='inline', div_id="throughput_chart")

    def _create_timing_distribution_chart(self, raw_results: List[Dict[str, Any]]) -> str:
        """Create timing distribution chart showing compilation vs scanning times."""
        if not PLOTLY_AVAILABLE:
            return "<p>Plotly not available</p>"

        # Extract timing data
        timing_data = []
        for result in raw_results:
            if (result and result.get('status') == 'ok' and
                result.get('compilation_ns') and result.get('scanning_ns')):
                timing_data.append({
                    'engine': result.get('engine_name', 'Unknown'),
                    'compilation_ms': result['compilation_ns'] / 1_000_000,
                    'scanning_ms': result['scanning_ns'] / 1_000_000,
                    'patterns': result.get('pattern_count', 0)
                })

        if len(timing_data) < 2:
            return "<p>Not enough timing data for distribution chart</p>"

        # Create stacked bar chart
        engines = {}
        for data in timing_data:
            engine = data['engine']
            if engine not in engines:
                engines[engine] = {'compilation': [], 'scanning': []}
            engines[engine]['compilation'].append(data['compilation_ms'])
            engines[engine]['scanning'].append(data['scanning_ms'])

        engine_names = list(engines.keys())
        avg_compilation = [sum(engines[engine]['compilation'])/len(engines[engine]['compilation'])
                          for engine in engine_names]
        avg_scanning = [sum(engines[engine]['scanning'])/len(engines[engine]['scanning'])
                       for engine in engine_names]

        fig = go.Figure(data=[
            go.Bar(name='Compilation Time', x=engine_names, y=avg_compilation,
                   marker_color='#e74c3c', opacity=0.8),
            go.Bar(name='Scanning Time', x=engine_names, y=avg_scanning,
                   marker_color='#f39c12', opacity=0.8)
        ])

        fig.update_layout(
            title="Average Timing Distribution (ms)",
            xaxis_title="Engine",
            yaxis_title="Time (ms)",
            barmode='stack',
            template="plotly_white",
            height=400,
            showlegend=True
        )

        return fig.to_html(include_plotlyjs='inline', div_id="timing_chart")

    def _create_engine_performance_chart(self, raw_results: List[Dict[str, Any]], analysis: Dict[str, Any]) -> str:
        """Create overall performance analysis chart."""
        if not PLOTLY_AVAILABLE:
            return "<p>Plotly not available</p>"

        # Extract performance metrics
        performance_data = []
        for result in raw_results:
            if (result and result.get('status') == 'ok' and
                result.get('scanning_ns') and result.get('corpus_size_bytes')):

                scanning_s = result['scanning_ns'] / 1_000_000_000
                corpus_mb = result['corpus_size_bytes'] / (1024 * 1024)
                throughput = corpus_mb / scanning_s if scanning_s > 0 else 0

                performance_data.append({
                    'engine': result.get('engine_name', 'Unknown'),
                    'throughput': throughput,
                    'patterns': result.get('pattern_count', 0),
                    'corpus_size': corpus_mb,
                    'match_count': result.get('match_count', 0)
                })

        if len(performance_data) < 2:
            return "<p>Not enough performance data for analysis chart</p>"

        # Create scatter plot showing throughput vs pattern count
        engines = {}
        for data in performance_data:
            engine = data['engine']
            if engine not in engines:
                engines[engine] = {'x': [], 'y': [], 'text': []}
            engines[engine]['x'].append(data['patterns'])
            engines[engine]['y'].append(data['throughput'])
            engines[engine]['text'].append(
                f"Corpus: {data['corpus_size']:.1f}MB<br>"
                f"Matches: {data['match_count']}"
            )

        fig = go.Figure()
        colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']

        for i, (engine, data) in enumerate(engines.items()):
            fig.add_trace(go.Scatter(
                x=data['x'],
                y=data['y'],
                mode='markers',
                name=engine,
                text=data['text'],
                hovertemplate=f'<b>{engine}</b><br>Patterns: %{{x}}<br>Throughput: %{{y:.2f}} MB/s<br>%{{text}}<extra></extra>',
                marker=dict(
                    size=10,
                    color=colors[i % len(colors)],
                    opacity=0.7
                )
            ))

        fig.update_layout(
            title="Performance Analysis: Throughput vs Pattern Count",
            xaxis_title="Pattern Count",
            yaxis_title="Throughput (MB/s)",
            template="plotly_white",
            height=400,
            showlegend=True
        )

        return fig.to_html(include_plotlyjs='inline', div_id="performance_chart")

    def _generate_raw_data_section(self, raw_results: List[Dict[str, Any]]) -> str:
        """Generate raw data section."""
        return f"""
        <section class="raw-data">
            <h2>ğŸ” Raw Data</h2>
            <details>
                <summary>Click to expand raw benchmark data</summary>
                <pre class="raw-json">{json.dumps(raw_results, indent=2)}</pre>
            </details>
        </section>"""

    def _get_css_styles(self) -> str:
        """Get CSS styles for the HTML report."""
        return """
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f8f9fa;
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        header {
            text-align: center;
            margin-bottom: 30px;
            padding: 20px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        header h1 {
            margin: 0;
            color: #2c3e50;
        }
        .metadata {
            margin-top: 10px;
            color: #666;
            display: flex;
            justify-content: center;
            gap: 20px;
        }
        section {
            background: white;
            margin: 20px 0;
            padding: 25px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h2 {
            color: #2c3e50;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 10px;
        }
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }
        .summary-card {
            text-align: center;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 6px;
            border-left: 4px solid #3498db;
        }
        .card-label {
            font-size: 12px;
            text-transform: uppercase;
            color: #666;
            margin-bottom: 5px;
        }
        .card-value {
            font-size: 24px;
            font-weight: bold;
            color: #2c3e50;
        }
        .status-success {
            color: #27ae60 !important;
        }
        .status-error {
            color: #e74c3c !important;
        }
        .table-container {
            overflow-x: auto;
        }
        .results-table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 15px;
        }
        .results-table th,
        .results-table td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ecf0f1;
        }
        .results-table th {
            background: #f8f9fa;
            font-weight: 600;
            color: #2c3e50;
        }
        .results-table tr:hover {
            background: #f8f9fa;
        }
        .engine-name {
            font-weight: 600;
            color: #3498db;
        }
        .performance-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
        }
        .performance-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            border-left: 4px solid #9b59b6;
        }
        .throughput-summary {
            margin-top: 20px;
            padding: 20px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .throughput-summary h3 {
            margin: 0 0 15px 0;
            color: #2c3e50;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 10px;
        }
        .throughput-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin-top: 15px;
        }
        .throughput-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            border-left: 4px solid #e74c3c;
        }
        .throughput-card h4 {
            margin: 0 0 15px 0;
            color: #2c3e50;
            font-weight: 600;
            text-align: center;
        }
        .throughput-stats {
            display: flex;
            flex-direction: column;
            gap: 8px;
        }
        .stat-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 8px 12px;
            background: white;
            border-radius: 4px;
            border: 1px solid #ecf0f1;
        }
        .stat-label {
            color: #666;
            font-weight: 500;
        }
        .stat-value {
            color: #2c3e50;
            font-weight: 700;
            font-size: 14px;
        }
        .performance-card h4 {
            margin: 0 0 15px 0;
            color: #2c3e50;
        }
        .perf-item {
            display: flex;
            justify-content: space-between;
            margin: 8px 0;
        }
        .perf-item .label {
            color: #666;
        }
        .perf-item .value {
            font-weight: 600;
        }
        .chart-placeholder {
            text-align: center;
            padding: 40px;
            background: #f8f9fa;
            border-radius: 6px;
            color: #666;
        }
        .raw-json {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 4px;
            overflow-x: auto;
            font-size: 12px;
            white-space: pre-wrap;
        }
        footer {
            text-align: center;
            margin-top: 40px;
            color: #666;
            font-size: 14px;
        }
        footer a {
            color: #3498db;
            text-decoration: none;
        }

        /* Chart Styles */
        .charts {
            background: white;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .chart-grid {
            display: grid;
            grid-template-columns: 1fr;
            gap: 30px;
        }
        .chart-container {
            background: #fafafa;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            padding: 20px;
            margin: 10px 0;
        }
        .chart-container h3 {
            margin: 0 0 15px 0;
            color: #2c3e50;
            font-size: 1.2em;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        .chart-placeholder {
            text-align: center;
            padding: 40px;
            color: #666;
            background: #f8f9fa;
            border-radius: 8px;
            margin: 20px 0;
        }
        .chart-error {
            text-align: center;
            padding: 20px;
            color: #e74c3c;
            background: #fff5f5;
            border: 1px solid #fccfcf;
            border-radius: 8px;
            margin: 20px 0;
        }
        /* Responsive chart grid for larger screens */
        @media (min-width: 1024px) {
            .chart-grid {
                grid-template-columns: 1fr 1fr;
            }
            .chart-container:last-child {
                grid-column: 1 / -1;
            }
        }
        """

    def _get_chart_scripts(self) -> str:
        """Get chart library scripts (placeholder)."""
        return """
        <!-- Chart.js would be included here -->
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        """

    def generate_enhanced_report(
        self,
        data: Dict[str, Any],
        output_dir: Path,
        include_charts: bool = False
    ) -> Path:
        """Generate an HTML report using enhanced database metadata for accurate job counts."""

        # Extract enhanced data structure
        metadata = data.get('metadata')
        benchmark_data = data.get('benchmark_results')

        # Create enhanced summary using database metadata for accurate job counts
        enhanced_summary = {}
        if metadata:
            enhanced_summary.update({
                'run_id': metadata.run_id,
                'total_combinations': metadata.total_jobs,  # Used by HTML formatter for Total Runs
                'successful_runs': metadata.completed_jobs,
                'failed_runs': metadata.failed_jobs,
                'queued': metadata.queued_jobs,
                'running': metadata.running_jobs,
                'skipped_lowvariance': metadata.total_jobs - metadata.completed_jobs - metadata.failed_jobs - metadata.queued_jobs - metadata.running_jobs,
                'status': metadata.status,
                'phase': 'production_fast_engines',  # Add phase info
                'created_at': metadata.created_at,
                'started_at': metadata.started_at,
                'completed_at': metadata.completed_at,
                'duration_seconds': metadata.duration_seconds if metadata.duration_seconds is not None else 0,
                'actual_execution_duration': metadata.actual_execution_duration if metadata.actual_execution_duration is not None else 'N/A',
                'config_path': metadata.config_path,
                'created_by': metadata.created_by,
            })

            # Add test matrix info if available
            if metadata.test_matrix:
                enhanced_summary['test_matrix'] = metadata.test_matrix

        # Merge with benchmark data (traditional or database-sourced)
        if benchmark_data:
            # Check if this is database-sourced data (has 'results' key) or traditional data (has 'raw_results' key)
            if 'results' in benchmark_data and 'raw_results' not in benchmark_data:
                # Database-sourced data - convert to expected format
                enhanced_data = {
                    'summary': {**benchmark_data.get('summary', {}), **enhanced_summary},
                    'raw_results': benchmark_data.get('results', []),
                    'engines': benchmark_data.get('engines', {}),
                    'analysis': {},
                    'metadata': {
                        'generated_at': data.get('generated_at'),
                        'database_path': data.get('database_path'),
                        'enhanced_reporting': True,
                        'data_source': 'database'
                    }
                }
            else:
                # Traditional data structure - use as-is but override job counts with database metadata
                enhanced_data = benchmark_data.copy()
                enhanced_data['summary'] = {**benchmark_data.get('summary', {}), **enhanced_summary}
                if 'metadata' not in enhanced_data:
                    enhanced_data['metadata'] = {}
                enhanced_data['metadata'].update({
                    'generated_at': data.get('generated_at'),
                    'database_path': data.get('database_path'),
                    'enhanced_reporting': True,
                    'data_source': 'traditional'
                })
        else:
            # Create minimal data structure from metadata only
            enhanced_data = {
                'summary': enhanced_summary,
                'raw_results': [],
                'analysis': {},
                'metadata': {
                    'generated_at': data.get('generated_at'),
                    'database_path': data.get('database_path'),
                    'enhanced_reporting': True,
                    'data_source': 'metadata_only'
                }
            }

        # Generate report using existing method with enhanced data
        return self.generate_report(enhanced_data, output_dir, include_charts)

    def _copy_assets(self, output_dir: Path) -> None:
        """Copy any additional assets needed for the report."""
        # Placeholder for copying CSS, JS, images, etc.
        pass


class MarkdownFormatter:
    """Generate Markdown reports."""

    def generate_report(
        self,
        data: Dict[str, Any],
        output_dir: Path,
        include_charts: bool = False
    ) -> Path:
        """Generate a Markdown report."""

        summary = data.get('summary', {})
        raw_results = data.get('raw_results', [])
        analysis = data.get('analysis', {})

        markdown = f"""# ğŸš€ Regex Benchmark Report

**Run ID:** {summary.get('run_id', 'Unknown')}
**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š Summary

- **Phase:** {str(summary.get('phase', 'unknown')).title()}
- **Status:** {str(summary.get('status', 'unknown')).title()}
- **Duration:** {summary.get('duration_seconds', 0):.2f}s
- **Engines:** {', '.join(summary.get('engines_tested', []))}
- **Total Runs:** {summary.get('total_combinations', 0)}
- **Success Rate:** {(summary.get('successful_runs', 0) / max(summary.get('total_combinations', 1), 1) * 100):.1f}%

## ğŸ”§ Results

| Engine | Status | Compilation (ms) | Scanning (ms) | Matches | Patterns | Regex/sec | Total Throughput |
|--------|--------|------------------|---------------|---------|----------|-----------|------------------|
"""

        for result in raw_results[:10]:  # Show first 10 results
            # Skip None results (incomplete jobs)
            if result is None:
                continue

            comp_ms = ((result.get('compilation_ns') or 0) / 1_000_000)
            scan_ms = ((result.get('scanning_ns') or 0) / 1_000_000)

            # Calculate pattern processing rate (regex/sec)
            patterns_per_sec_display = "-"
            patterns_compiled = result.get('patterns_compiled', 0)
            scanning_ns = result.get('scanning_ns', 0)
            if scanning_ns and scanning_ns > 0 and patterns_compiled and patterns_compiled > 0:
                scanning_seconds = scanning_ns / 1_000_000_000  # Convert nanoseconds to seconds
                patterns_per_sec = patterns_compiled / scanning_seconds
                if patterns_per_sec >= 1000:
                    patterns_per_sec_display = f"{patterns_per_sec/1000:.1f}K/sec"
                else:
                    patterns_per_sec_display = f"{patterns_per_sec:.1f}/sec"

            # Calculate total pattern throughput (patterns Ã— MB/sec)
            total_throughput_display = "-"
            corpus_size_bytes = result.get('corpus_size_bytes', 0)
            corpus_size_mb = corpus_size_bytes / (1024 * 1024) if corpus_size_bytes and corpus_size_bytes > 0 else 0
            if scanning_ns and scanning_ns > 0 and patterns_compiled and patterns_compiled > 0 and corpus_size_mb > 0:
                scanning_seconds = scanning_ns / 1_000_000_000  # Convert nanoseconds to seconds
                total_throughput = (patterns_compiled * corpus_size_mb) / scanning_seconds
                if total_throughput >= 1000:
                    total_throughput_display = f"{total_throughput/1000:.1f}K pâ‹…MB/s"
                else:
                    total_throughput_display = f"{total_throughput:.1f} pâ‹…MB/s"

            markdown += f"| {result.get('engine_name', 'Unknown')} | {result.get('status', 'unknown')} | {comp_ms:.3f} | {scan_ms:.3f} | {result.get('match_count', 0)} | {result.get('patterns_compiled', 0)} | {patterns_per_sec_display} | {total_throughput_display} |\n"

        if len(raw_results) > 10:
            markdown += f"\n*... and {len(raw_results) - 10} more results*\n"

        markdown += f"""

## ğŸ“„ Raw Data

```json
{json.dumps(summary, indent=2)}
```

---
*Generated by Regex Bench Framework v2.0*"""

        # Write markdown file
        report_file = output_dir / "benchmark_report.md"
        with open(report_file, 'w') as f:
            f.write(markdown)

        return report_file