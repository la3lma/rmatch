"""
Report formatters for different output formats.
"""

import json
import time
import sys
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

# Chart generation libraries
try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
    print("DEBUG: Plotly import successful in formatter")
except ImportError as e:
    PLOTLY_AVAILABLE = False
    print(f"DEBUG: Plotly import failed in formatter: {e}")
except Exception as e:
    PLOTLY_AVAILABLE = False
    print(f"DEBUG: Plotly other error in formatter: {e}")

# Performance analysis imports - conditional to avoid psutil dependency issues
ANALYSIS_AVAILABLE = False
try:
    # Try to import analysis modules without triggering full package imports
    sys.path.append(str(Path(__file__).parent.parent))

    # Use the standalone analysis approach to avoid psutil dependency
    standalone_analysis_path = Path(__file__).parent.parent.parent / "standalone_analysis.py"
    if standalone_analysis_path.exists():
        ANALYSIS_AVAILABLE = "standalone"
    else:
        # Fallback to try full analysis if psutil is working
        from analysis.performance_analyzer import PerformanceAnalyzer
        from analysis.chart_generator import ChartGenerator
        ANALYSIS_AVAILABLE = True
except ImportError:
    # psutil dependency issue - use standalone approach
    ANALYSIS_AVAILABLE = False


class HTMLFormatter:
    """Generate HTML reports with interactive features."""

    def generate_report(
        self,
        data: Dict[str, Any],
        output_dir: Path,
        include_charts: bool = False
    ) -> Path:
        """Generate an HTML report."""

        # Generate report content
        html_content = self._generate_html_report(data, include_charts)

        # Write report file
        report_file = output_dir / "benchmark_report.html"
        with open(report_file, 'w') as f:
            f.write(html_content)

        # Copy any assets if needed
        self._copy_assets(output_dir)

        return report_file

    def _generate_html_report(self, data: Dict[str, Any], include_charts: bool) -> str:
        """Generate the complete HTML report content."""

        summary = data.get('summary', {})
        raw_results = data.get('raw_results', [])
        analysis = data.get('analysis', {})
        metadata = data.get('metadata', {})

        html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Regex Benchmark Report - {summary.get('run_id', 'Unknown')}</title>
    <style>
        {self._get_css_styles()}
    </style>
    {self._get_chart_scripts() if include_charts else ''}
</head>
<body>
    <div class="container">
        <header>
            <h1>üöÄ Regex Benchmark Report</h1>
            <div class="metadata">
                <span class="run-id">Run ID: {summary.get('run_id', 'Unknown')}</span>
                <span class="timestamp">Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</span>
            </div>
        </header>

        {self._generate_summary_section(summary, analysis)}
        {self._generate_engines_section(raw_results)}
        {self._generate_results_section(raw_results, analysis)}
        {self._generate_performance_section(analysis, raw_results, data.get('metadata', {}).get('database_path'))}
        {'    ' + self._generate_charts_section(raw_results, analysis, data.get('metadata', {}).get('database_path')) if include_charts else ''}
        {self._generate_raw_data_section(raw_results)}

        <footer>
            <p>Generated by Regex Bench Framework v2.0 ‚Ä¢ <a href="https://github.com/anthropics/claude-code">Powered by Claude Code</a></p>
        </footer>
    </div>
</body>
</html>"""

        return html

    def _generate_summary_section(self, summary: Dict[str, Any], analysis: Dict[str, Any] = None) -> str:
        """Generate the summary section with throughput statistics."""
        phase = str(summary.get('phase', 'unknown'))
        status = str(summary.get('status', 'unknown'))
        engines_tested = summary.get('engines_tested', [])
        engines = ', '.join([str(e) for e in engines_tested]) if engines_tested else 'None'
        total = int(summary.get('total_combinations', 0))
        successful = int(summary.get('successful_runs', 0))
        failed = int(summary.get('failed_runs', 0))
        duration = float(summary.get('duration_seconds', 0))

        status_class = 'status-success' if status.lower() == 'completed' else 'status-error'

        # Build main summary cards
        summary_cards = f"""
                <div class="summary-card">
                    <div class="card-label">Phase</div>
                    <div class="card-value">{phase.title()}</div>
                </div>
                <div class="summary-card">
                    <div class="card-label">Status</div>
                    <div class="card-value {status_class}">{status.title()}</div>
                </div>
                <div class="summary-card">
                    <div class="card-label">Duration</div>
                    <div class="card-value">{duration:.2f}s</div>
                </div>
                <div class="summary-card">
                    <div class="card-label">Engines Tested</div>
                    <div class="card-value">{engines}</div>
                </div>
                <div class="summary-card">
                    <div class="card-label">Total Runs</div>
                    <div class="card-value">{total}</div>
                </div>
                <div class="summary-card">
                    <div class="card-label">Success Rate</div>
                    <div class="card-value">{(successful/max(total,1)*100):.1f}%</div>
                </div>"""

        # Add throughput statistics if available
        throughput_section = ""
        if analysis and 'summary' in analysis and 'throughput_summary' in analysis['summary']:
            throughput_data = analysis['summary']['throughput_summary']
            if throughput_data:
                throughput_section = """
            <div class="throughput-summary">
                <h3>üöÄ Throughput Performance Summary</h3>
                <div class="throughput-grid">"""

                for engine_name, stats in throughput_data.items():
                    throughput_section += f"""
                    <div class="throughput-card">
                        <h4>{engine_name}</h4>
                        <div class="throughput-stats">
                            <div class="stat-item">
                                <span class="stat-label">Average:</span>
                                <span class="stat-value">{stats['average_mean_mbps']:.2f} MB/s</span>
                            </div>
                            <div class="stat-item">
                                <span class="stat-label">Median:</span>
                                <span class="stat-value">{stats['average_median_mbps']:.2f} MB/s</span>
                            </div>
                            <div class="stat-item">
                                <span class="stat-label">Std Dev:</span>
                                <span class="stat-value">{stats['average_std_dev_mbps']:.2f} MB/s</span>
                            </div>
                            <div class="stat-item">
                                <span class="stat-label">Peak:</span>
                                <span class="stat-value">{stats['best_mean_mbps']:.2f} MB/s</span>
                            </div>
                        </div>
                    </div>"""

                throughput_section += """
                </div>
            </div>"""

        return f"""
        <section class="summary">
            <h2>üìä Benchmark Summary</h2>
            <div class="summary-grid">
                {summary_cards}
            </div>
            {throughput_section}
        </section>"""

    def _generate_engines_section(self, raw_results: List[Dict[str, Any]]) -> str:
        """Generate the engines overview section."""
        # Group results by engine
        engines = {}
        for result in raw_results:
            # Skip None results (incomplete jobs)
            if result is None:
                continue

            engine_name = result.get('engine_name', 'unknown')
            if engine_name not in engines:
                engines[engine_name] = {
                    'name': engine_name,
                    'runs': 0,
                    'successful_runs': 0,
                    'avg_compilation_ns': 0,
                    'avg_scanning_ns': 0,
                    'total_matches': 0
                }

            engines[engine_name]['runs'] += 1
            if result.get('status') == 'ok':
                engines[engine_name]['successful_runs'] += 1
                # Handle None values that might come from incomplete jobs
                compilation_ns = result.get('compilation_ns', 0) or 0
                scanning_ns = result.get('scanning_ns', 0) or 0
                match_count = result.get('match_count', 0) or 0
                engines[engine_name]['avg_compilation_ns'] += compilation_ns
                engines[engine_name]['avg_scanning_ns'] += scanning_ns
                engines[engine_name]['total_matches'] += match_count

        # Calculate averages
        for engine in engines.values():
            if engine['successful_runs'] > 0:
                engine['avg_compilation_ns'] //= engine['successful_runs']
                engine['avg_scanning_ns'] //= engine['successful_runs']

        engine_rows = ""
        for engine in engines.values():
            success_rate = (engine['successful_runs'] / max(engine['runs'], 1)) * 100
            engine_rows += f"""
                <tr>
                    <td class="engine-name">{engine['name']}</td>
                    <td>{engine['runs']}</td>
                    <td>{success_rate:.1f}%</td>
                    <td>{engine['avg_compilation_ns']:,} ns</td>
                    <td>{engine['avg_scanning_ns']:,} ns</td>
                    <td>{engine['total_matches']}</td>
                </tr>"""

        return f"""
        <section class="engines">
            <h2>üîß Engine Performance Overview</h2>
            <div class="table-container">
                <table class="results-table">
                    <thead>
                        <tr>
                            <th>Engine</th>
                            <th>Total Runs</th>
                            <th>Success Rate</th>
                            <th>Avg Compilation</th>
                            <th>Avg Scanning</th>
                            <th>Total Matches</th>
                        </tr>
                    </thead>
                    <tbody>
                        {engine_rows}
                    </tbody>
                </table>
            </div>
        </section>"""

    def _generate_results_section(self, raw_results: List[Dict[str, Any]], analysis: Dict[str, Any]) -> str:
        """Generate detailed results section."""
        if not raw_results:
            return "<section><h2>üìã Results</h2><p>No results available.</p></section>"

        result_rows = ""
        for i, result in enumerate(raw_results):
            # Skip None results (incomplete jobs)
            if result is None:
                continue

            status_class = 'status-success' if result.get('status') == 'ok' else 'status-error'
            compilation_ms = ((result.get('compilation_ns') or 0) / 1_000_000)
            scanning_ms = ((result.get('scanning_ns') or 0) / 1_000_000)

            # Calculate corpus size and MB/sec throughput
            corpus_size_bytes = result.get('corpus_size_bytes') or 0
            corpus_size_mb = corpus_size_bytes / (1024 * 1024) if corpus_size_bytes and corpus_size_bytes > 0 else 0

            # Calculate MB/sec scanning throughput
            scanning_ns = result.get('scanning_ns') or 0
            mb_per_sec = 0
            if scanning_ns and scanning_ns > 0 and corpus_size_bytes and corpus_size_bytes > 0:
                scanning_seconds = scanning_ns / 1_000_000_000  # Convert nanoseconds to seconds
                mb_per_sec = corpus_size_mb / scanning_seconds

            # Calculate pattern processing rate (regex/sec)
            patterns_per_sec_display = "-"
            patterns_compiled = result.get('patterns_compiled', 0)
            if scanning_ns and scanning_ns > 0 and patterns_compiled and patterns_compiled > 0:
                scanning_seconds = scanning_ns / 1_000_000_000  # Convert nanoseconds to seconds
                patterns_per_sec = patterns_compiled / scanning_seconds
                if patterns_per_sec >= 1000:
                    patterns_per_sec_display = f"{patterns_per_sec/1000:.1f}K/sec"
                else:
                    patterns_per_sec_display = f"{patterns_per_sec:.1f}/sec"

            # Calculate total pattern throughput (patterns √ó MB/sec)
            total_throughput_display = "-"
            if scanning_ns and scanning_ns > 0 and patterns_compiled and patterns_compiled > 0 and corpus_size_mb > 0:
                scanning_seconds = scanning_ns / 1_000_000_000  # Convert nanoseconds to seconds
                total_throughput = (patterns_compiled * corpus_size_mb) / scanning_seconds
                if total_throughput >= 1000:
                    total_throughput_display = f"{total_throughput/1000:.1f}K p‚ãÖMB/s"
                else:
                    total_throughput_display = f"{total_throughput:.1f} p‚ãÖMB/s"

            result_rows += f"""
                <tr class="result-row">
                    <td>{i + 1}</td>
                    <td>{result.get('engine_name', 'Unknown')}</td>
                    <td>{result.get('iteration', 0)}</td>
                    <td class="{status_class}">{result.get('status', 'unknown')}</td>
                    <td>{corpus_size_mb:.1f} MB</td>
                    <td>{compilation_ms:.3f}</td>
                    <td>{scanning_ms:.3f}</td>
                    <td>{mb_per_sec:.2f} MB/sec</td>
                    <td>{result.get('match_count', 0)}</td>
                    <td>{result.get('patterns_compiled', 0)}</td>
                    <td>{patterns_per_sec_display}</td>
                    <td>{total_throughput_display}</td>
                </tr>"""

        return f"""
        <section class="detailed-results">
            <h2>üìã Detailed Results</h2>
            <div class="table-container">
                <table class="results-table">
                    <thead>
                        <tr>
                            <th>#</th>
                            <th>Engine</th>
                            <th>Iteration</th>
                            <th>Status</th>
                            <th>Corpus Size</th>
                            <th>Compilation (ms)</th>
                            <th>Scanning (ms)</th>
                            <th>Throughput</th>
                            <th>Matches</th>
                            <th>Patterns</th>
                            <th>Regex/sec</th>
                            <th>Total Throughput</th>
                        </tr>
                    </thead>
                    <tbody>
                        {result_rows}
                    </tbody>
                </table>
            </div>
        </section>"""

    def _generate_performance_section(self, analysis: Dict[str, Any], raw_results: List[Dict[str, Any]] = None, db_path: Optional[str] = None) -> str:
        """Generate enhanced performance analysis section with memory usage."""
        grouped_stats = analysis.get('grouped_statistics', {})
        comparisons = analysis.get('comparisons', {})

        # Try to generate comprehensive memory analysis if we have access to the database
        memory_analysis_section = ""
        if ANALYSIS_AVAILABLE and db_path and Path(db_path).exists():
            try:
                memory_analysis_section = self._generate_memory_analysis(db_path)
            except Exception as e:
                print(f"Warning: Could not generate memory analysis: {e}")

        # Generate basic performance stats from existing data
        stats_content = ""
        if grouped_stats:
            for group_key, stats in grouped_stats.items():
                engine_name = stats.get('engine_name', 'Unknown')
                patterns = stats.get('patterns_compiled', 0)
                corpus_size = stats.get('corpus_size_bytes', 0)

                # Get scanning statistics
                scanning = stats.get('scanning', {})
                if scanning:
                    mean_ms = (scanning.get('mean') or 0) / 1_000_000
                    median_ms = (scanning.get('median') or 0) / 1_000_000
                    std_ms = (scanning.get('std_dev') or 0) / 1_000_000

                    stats_content += f"""
                    <div class="performance-card">
                        <h4>{engine_name}</h4>
                        <div class="perf-details">
                            <div class="perf-item">
                                <span class="label">Patterns:</span>
                                <span class="value">{patterns}</span>
                            </div>
                            <div class="perf-item">
                                <span class="label">Corpus Size:</span>
                                <span class="value">{corpus_size:,} bytes</span>
                            </div>
                            <div class="perf-item">
                                <span class="label">Mean Scan Time:</span>
                                <span class="value">{mean_ms:.3f} ms</span>
                            </div>
                            <div class="perf-item">
                                <span class="label">Median Scan Time:</span>
                                <span class="value">{median_ms:.3f} ms</span>
                            </div>
                            <div class="perf-item">
                                <span class="label">Std Deviation:</span>
                                <span class="value">{std_ms:.3f} ms</span>
                            </div>
                        </div>
                    </div>"""

        # Generate memory overview from raw results if available
        memory_overview_section = ""
        if raw_results:
            memory_overview_section = self._generate_memory_overview_from_results(raw_results)

        performance_insights = """
        <div class="insights-section">
            <h3>üß† Performance Insights</h3>
            <div class="insight-cards">
                <div class="insight-card">
                    <h4>Pattern Complexity Impact</h4>
                    <p>Performance scales with pattern count. Complex patterns with lookarounds and character classes show higher compilation overhead.</p>
                </div>
                <div class="insight-card">
                    <h4>Corpus Size Scaling</h4>
                    <p>Linear scaling expected for scanning time. Memory usage may vary based on engine optimization strategies.</p>
                </div>
                <div class="insight-card">
                    <h4>Engine Trade-offs</h4>
                    <p>Different engines show varying memory vs performance trade-offs. Monitor both metrics for optimal selection.</p>
                </div>
            </div>
        </div>"""

        basic_performance = f"""
        <div class="performance-grid">
            {stats_content}
        </div>""" if stats_content else "<p>No traditional performance statistics available.</p>"

        return f"""
        <section class="performance">
            <h2>‚ö° Performance Analysis</h2>
            {performance_insights}
            {basic_performance}
            {memory_overview_section}
            {memory_analysis_section}
        </section>"""

    def _generate_memory_analysis(self, db_path: str) -> str:
        """Generate comprehensive memory analysis from database."""
        try:
            # Use standalone analysis approach to avoid psutil issues
            if ANALYSIS_AVAILABLE == "standalone":
                return self._generate_memory_analysis_standalone(db_path)
            elif ANALYSIS_AVAILABLE:
                # Full analysis available
                analyzer = PerformanceAnalyzer(Path(db_path))
                memory_analysis = analyzer.analyze_memory_usage()
                memory_summary = analyzer.get_memory_comparison_summary()
            else:
                return self._generate_memory_analysis_from_db(db_path)

            # Generate memory insights
            insights = []
            memory_overview = memory_summary.get('engine_memory_overview', {})
            if memory_overview:
                min_memory_engine = min(memory_overview.keys(),
                                      key=lambda e: memory_overview[e]['avg_memory_mb'])
                max_memory_engine = max(memory_overview.keys(),
                                      key=lambda e: memory_overview[e]['avg_memory_mb'])

                min_mem = memory_overview[min_memory_engine]['avg_memory_mb']
                max_mem = memory_overview[max_memory_engine]['avg_memory_mb']

                insights.append(f"{min_memory_engine} is most memory-efficient (avg: {min_mem:.1f} MB)")
                insights.append(f"{max_memory_engine} uses most memory (avg: {max_mem:.1f} MB)")
                insights.append(f"Memory usage varies by {max_mem/min_mem:.1f}x between engines")

            # Generate engine memory cards
            memory_cards = ""
            for engine, stats in memory_overview.items():
                memory_cards += f"""
                <div class="memory-card">
                    <h4>{engine}</h4>
                    <div class="memory-stats">
                        <div class="memory-stat">
                            <span class="label">Average:</span>
                            <span class="value">{stats['avg_memory_mb']:.1f} MB</span>
                        </div>
                        <div class="memory-stat">
                            <span class="label">Peak:</span>
                            <span class="value">{stats['max_memory_mb']:.1f} MB</span>
                        </div>
                        <div class="memory-stat">
                            <span class="label">Minimum:</span>
                            <span class="value">{stats['min_memory_mb']:.1f} MB</span>
                        </div>
                    </div>
                </div>"""

            insights_html = ""
            if insights:
                insights_html = """
                <div class="memory-insights">
                    <h4>üîç Memory Usage Insights</h4>
                    <ul class="insights-list">""" + \
                    "".join([f"<li>{insight}</li>" for insight in insights]) + \
                    """</ul>
                </div>"""

            return f"""
            <div class="memory-analysis-section">
                <h3>üß† Memory Usage Analysis</h3>
                {insights_html}
                <div class="memory-grid">
                    {memory_cards}
                </div>
            </div>"""

        except Exception as e:
            return f"""
            <div class="memory-analysis-error">
                <h3>üß† Memory Usage Analysis</h3>
                <p>Could not load comprehensive memory analysis: {e}</p>
            </div>"""

    def _generate_memory_analysis_standalone(self, db_path: str) -> str:
        """Generate memory analysis using standalone script to avoid psutil dependency."""
        try:
            import subprocess
            import tempfile

            # Use standalone analysis script
            standalone_script = Path(__file__).parent.parent.parent / "standalone_analysis.py"

            # Create temporary output directory
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_output = Path(temp_dir)

                # Run standalone analysis
                result = subprocess.run([
                    "python3", str(standalone_script),
                    str(db_path),
                    "--output-dir", str(temp_output)
                ], capture_output=True, text=True, timeout=30)

                if result.returncode == 0:
                    # Load the analysis summary
                    summary_file = temp_output / "analysis_summary.json"
                    if summary_file.exists():
                        with open(summary_file) as f:
                            summary_data = json.load(f)

                        return self._format_memory_analysis_from_summary(summary_data)
                    else:
                        return """
                        <div class="memory-analysis-error">
                            <h3>üß† Memory Usage Analysis</h3>
                            <p>Standalone analysis completed but summary not found</p>
                        </div>"""
                else:
                    return f"""
                    <div class="memory-analysis-error">
                        <h3>üß† Memory Usage Analysis</h3>
                        <p>Standalone analysis failed: {result.stderr}</p>
                    </div>"""

        except Exception as e:
            return f"""
            <div class="memory-analysis-error">
                <h3>üß† Memory Usage Analysis</h3>
                <p>Could not run standalone memory analysis: {e}</p>
            </div>"""

    def _generate_memory_analysis_from_db(self, db_path: str) -> str:
        """Generate basic memory analysis directly from database without dependencies."""
        try:
            import sqlite3

            conn = sqlite3.connect(db_path)
            cursor = conn.execute("""
                SELECT
                    engine_name,
                    AVG(memory_compilation_bytes) / (1024*1024) as avg_memory_mb,
                    MAX(memory_compilation_bytes) / (1024*1024) as max_memory_mb,
                    MIN(memory_compilation_bytes) / (1024*1024) as min_memory_mb,
                    AVG(memory_compilation_bytes / NULLIF(pattern_count, 0)) / 1024 as avg_memory_per_pattern_kb,
                    MAX(memory_compilation_bytes / NULLIF(pattern_count, 0)) / 1024 as max_memory_per_pattern_kb,
                    AVG(pattern_count) as avg_pattern_count,
                    COUNT(*) as sample_count
                FROM benchmark_jobs
                WHERE status = 'COMPLETED'
                AND memory_compilation_bytes IS NOT NULL
                AND pattern_count > 0
                GROUP BY engine_name
                ORDER BY avg_memory_per_pattern_kb
            """)

            memory_data = cursor.fetchall()
            conn.close()

            if not memory_data:
                return """
                <div class="memory-analysis-error">
                    <h3>üß† Memory Usage Analysis</h3>
                    <p>No memory data available in database</p>
                </div>"""

            # Generate efficiency-focused insights
            min_engine = memory_data[0]  # First row has best efficiency (lowest memory per pattern)
            max_engine = memory_data[-1]  # Last row has worst efficiency (highest memory per pattern)

            insights = [
                f"{min_engine[0]} is most memory-efficient ({min_engine[4]:.1f} KB per regex)",
                f"{max_engine[0]} is least memory-efficient ({max_engine[4]:.1f} KB per regex)",
                f"Memory efficiency varies by {max_engine[4]/min_engine[4]:.1f}x between engines",
                f"Engines tested with avg {min_engine[6]:.0f}-{max_engine[6]:.0f} patterns per job"
            ]

            # Generate enhanced memory cards with efficiency metrics
            memory_cards = ""
            for engine_name, avg_mb, max_mb, min_mb, avg_kb_per_pattern, max_kb_per_pattern, avg_patterns, samples in memory_data:
                memory_cards += f"""
                <div class="memory-card">
                    <h4>{engine_name}</h4>
                    <div class="memory-stats">
                        <div class="memory-stat efficiency-highlight">
                            <span class="label">Memory per Regex:</span>
                            <span class="value">{avg_kb_per_pattern:.1f} KB</span>
                        </div>
                        <div class="memory-stat">
                            <span class="label">Peak per Regex:</span>
                            <span class="value">{max_kb_per_pattern:.1f} KB</span>
                        </div>
                        <div class="memory-stat">
                            <span class="label">Total Average:</span>
                            <span class="value">{avg_mb:.1f} MB</span>
                        </div>
                        <div class="memory-stat">
                            <span class="label">Avg Patterns:</span>
                            <span class="value">{avg_patterns:.0f}</span>
                        </div>
                        <div class="memory-stat">
                            <span class="label">Samples:</span>
                            <span class="value">{samples}</span>
                        </div>
                    </div>
                </div>"""

            insights_html = """
            <div class="memory-insights">
                <h4>üîç Memory Usage Insights</h4>
                <ul class="insights-list">""" + \
                "".join([f"<li>{insight}</li>" for insight in insights]) + \
                """</ul>
            </div>"""

            return f"""
            <div class="memory-analysis-section">
                <h3>üß† Memory Usage Analysis</h3>
                {insights_html}
                <div class="memory-grid">
                    {memory_cards}
                </div>
            </div>"""

        except Exception as e:
            return f"""
            <div class="memory-analysis-error">
                <h3>üß† Memory Usage Analysis</h3>
                <p>Could not analyze memory from database: {e}</p>
            </div>"""

    def _format_memory_analysis_from_summary(self, summary_data: Dict[str, Any]) -> str:
        """Format memory analysis HTML from standalone analysis summary."""
        memory_overview = summary_data.get('memory_overview', {})
        if not memory_overview:
            return """
            <div class="memory-analysis-error">
                <h3>üß† Memory Usage Analysis</h3>
                <p>No memory overview data in analysis</p>
            </div>"""

        # Generate insights
        engines_by_memory = sorted(memory_overview.items(),
                                 key=lambda x: x[1]['avg_memory_mb'])
        min_engine = engines_by_memory[0]
        max_engine = engines_by_memory[-1]

        insights = [
            f"{min_engine[0]} is most memory-efficient (avg: {min_engine[1]['avg_memory_mb']:.1f} MB)",
            f"{max_engine[0]} uses most memory (avg: {max_engine[1]['avg_memory_mb']:.1f} MB)",
            f"Memory usage varies by {max_engine[1]['avg_memory_mb']/min_engine[1]['avg_memory_mb']:.1f}x between engines"
        ]

        # Generate memory cards
        memory_cards = ""
        for engine, stats in memory_overview.items():
            memory_cards += f"""
            <div class="memory-card">
                <h4>{engine}</h4>
                <div class="memory-stats">
                    <div class="memory-stat">
                        <span class="label">Average:</span>
                        <span class="value">{stats['avg_memory_mb']:.1f} MB</span>
                    </div>
                    <div class="memory-stat">
                        <span class="label">Peak:</span>
                        <span class="value">{stats['max_memory_mb']:.1f} MB</span>
                    </div>
                    <div class="memory-stat">
                        <span class="label">Minimum:</span>
                        <span class="value">{stats['min_memory_mb']:.1f} MB</span>
                    </div>
                </div>
            </div>"""

        insights_html = """
        <div class="memory-insights">
            <h4>üîç Memory Usage Insights</h4>
            <ul class="insights-list">""" + \
            "".join([f"<li>{insight}</li>" for insight in insights]) + \
            """</ul>
        </div>"""

        return f"""
        <div class="memory-analysis-section">
            <h3>üß† Memory Usage Analysis</h3>
            {insights_html}
            <div class="memory-grid">
                {memory_cards}
            </div>
        </div>"""

    def _generate_memory_overview_from_results(self, raw_results: List[Dict[str, Any]]) -> str:
        """Generate memory overview from raw results data."""
        # Extract memory data from raw results
        memory_data = {}

        for result in raw_results:
            if not result or result.get('status') != 'ok':
                continue

            engine = result.get('engine_name', 'Unknown')
            # Check for memory fields that might be in the result
            memory_bytes = None

            # Try different possible field names for memory data
            for field in ['memory_peak_bytes', 'memory_compilation_bytes', 'memory_bytes']:
                if field in result and result[field] is not None:
                    memory_bytes = result[field]
                    break

            if memory_bytes and memory_bytes > 0:
                if engine not in memory_data:
                    memory_data[engine] = []
                memory_data[engine].append(memory_bytes / (1024 * 1024))  # Convert to MB

        if not memory_data:
            return """
            <div class="memory-notice">
                <h3>üß† Memory Usage</h3>
                <p>Memory usage data not available in current results.</p>
            </div>"""

        # Generate memory overview cards
        memory_cards = ""
        for engine, memory_values in memory_data.items():
            avg_memory = sum(memory_values) / len(memory_values)
            max_memory = max(memory_values)
            min_memory = min(memory_values)

            memory_cards += f"""
            <div class="memory-overview-card">
                <h4>{engine}</h4>
                <div class="memory-overview-stats">
                    <div class="memory-stat">
                        <span class="label">Average:</span>
                        <span class="value">{avg_memory:.1f} MB</span>
                    </div>
                    <div class="memory-stat">
                        <span class="label">Peak:</span>
                        <span class="value">{max_memory:.1f} MB</span>
                    </div>
                    <div class="memory-stat">
                        <span class="label">Samples:</span>
                        <span class="value">{len(memory_values)}</span>
                    </div>
                </div>
            </div>"""

        return f"""
        <div class="memory-overview-section">
            <h3>üß† Memory Usage Overview</h3>
            <div class="memory-overview-grid">
                {memory_cards}
            </div>
        </div>"""

    def _generate_charts_section(self, raw_results: List[Dict[str, Any]], analysis: Dict[str, Any], db_path: Optional[str] = None) -> str:
        """Generate enhanced interactive charts section with memory analysis."""
        print(f"DEBUG: PLOTLY_AVAILABLE = {PLOTLY_AVAILABLE}")  # Debug line
        if not PLOTLY_AVAILABLE:
            return """
        <section class="charts">
            <h2>üìà Performance Charts</h2>
            <div class="chart-placeholder">
                <p>üìä Interactive charts require plotly library. Install with: pip install plotly</p>
                <p>Enhanced charts would include: throughput comparisons, memory usage analysis, pattern scaling, and performance heatmaps.</p>
            </div>
        </section>"""

        # Try to generate comprehensive charts from database if available
        comprehensive_charts = ""
        if ANALYSIS_AVAILABLE and db_path and Path(db_path).exists():
            try:
                comprehensive_charts = self._generate_comprehensive_charts(db_path)
            except Exception as e:
                print(f"Warning: Could not generate comprehensive charts: {e}")

        # Skip basic chart generation if insufficient data
        if not raw_results or len(raw_results) < 2:
            basic_charts_message = """
            <div class="chart-placeholder">
                <p>üìä Insufficient data for basic charts. Need at least 2 completed results.</p>
            </div>"""
        else:
            try:
                # Generate basic interactive charts
                throughput_chart = self._create_throughput_comparison_chart(raw_results)
                timing_chart = self._create_timing_distribution_chart(raw_results)
                performance_chart = self._create_engine_performance_chart(raw_results, analysis)
                memory_chart = self._create_memory_comparison_chart(raw_results)

                basic_charts_message = f"""
                <div class="chart-section">
                    <h3>Basic Performance Charts</h3>
                    <div class="basic-chart-grid">
                        <div class="chart-container">
                            <h4>üöÄ Engine Throughput Comparison</h4>
                            {throughput_chart}
                        </div>
                        <div class="chart-container">
                            <h4>‚è±Ô∏è Timing Distribution</h4>
                            {timing_chart}
                        </div>
                        <div class="chart-container">
                            <h4>üß† Memory Usage Comparison</h4>
                            {memory_chart}
                        </div>
                        <div class="chart-container">
                            <h4>üìä Overall Performance Analysis</h4>
                            {performance_chart}
                        </div>
                    </div>
                </div>"""
            except Exception as e:
                basic_charts_message = f"""
                <div class="chart-error">
                    <p>‚ùå Error generating basic charts: {str(e)}</p>
                    <p>Please check your data and try again.</p>
                </div>"""

        return f"""
        <section class="charts">
            <h2>üìà Performance Charts</h2>
            {comprehensive_charts}
            {basic_charts_message}
        </section>"""

    def _generate_comprehensive_charts(self, db_path: str) -> str:
        """Generate comprehensive interactive charts from database analysis."""
        try:
            analyzer = PerformanceAnalyzer(Path(db_path))
            chart_generator = ChartGenerator(analyzer)

            # Generate all comprehensive charts
            all_charts = chart_generator.generate_all_performance_charts()

            # Extract chart HTML for embedding
            charts_html = ""
            if all_charts:
                # Generate memory vs patterns charts for different corpus sizes
                memory_charts = ""
                corpus_sizes = ['1MB', '10MB', '100MB', '1GB']
                for corpus_size in corpus_sizes:
                    chart_key = f'memory_vs_patterns_{corpus_size}'
                    if chart_key in all_charts:
                        chart_config = all_charts[chart_key]
                        chart_html = self._plotly_config_to_html(chart_config, f'memory_patterns_{corpus_size}')
                        memory_charts += f"""
                        <div class="comprehensive-chart">
                            <h4>Memory vs Patterns ({corpus_size} Corpus)</h4>
                            {chart_html}
                        </div>"""

                # Generate throughput charts
                throughput_charts = ""
                for corpus_size in corpus_sizes:
                    chart_key = f'throughput_vs_patterns_{corpus_size}'
                    if chart_key in all_charts:
                        chart_config = all_charts[chart_key]
                        chart_html = self._plotly_config_to_html(chart_config, f'throughput_patterns_{corpus_size}')
                        throughput_charts += f"""
                        <div class="comprehensive-chart">
                            <h4>Throughput vs Patterns ({corpus_size} Corpus)</h4>
                            {chart_html}
                        </div>"""

                # Generate memory efficiency chart
                memory_efficiency_chart = ""
                if 'memory_efficiency_per_pattern' in all_charts:
                    chart_config = all_charts['memory_efficiency_per_pattern']
                    chart_html = self._plotly_config_to_html(chart_config, 'memory_efficiency')
                    memory_efficiency_chart = f"""
                    <div class="comprehensive-chart full-width">
                        <h4>Memory Efficiency Per Pattern</h4>
                        {chart_html}
                    </div>"""

                charts_html = f"""
                <div class="comprehensive-charts">
                    <h3>Comprehensive Performance Analysis</h3>
                    <div class="chart-tabs">
                        <button class="tab-button active" onclick="showChartTab('memory')">Memory Analysis</button>
                        <button class="tab-button" onclick="showChartTab('throughput')">Throughput Analysis</button>
                        <button class="tab-button" onclick="showChartTab('efficiency')">Efficiency Analysis</button>
                    </div>

                    <div id="memory-tab" class="chart-tab-content active">
                        <div class="comprehensive-chart-grid">
                            {memory_charts}
                        </div>
                    </div>

                    <div id="throughput-tab" class="chart-tab-content">
                        <div class="comprehensive-chart-grid">
                            {throughput_charts}
                        </div>
                    </div>

                    <div id="efficiency-tab" class="chart-tab-content">
                        {memory_efficiency_chart}
                    </div>
                </div>

                <script>
                function showChartTab(tabName) {{
                    // Hide all tab contents
                    document.querySelectorAll('.chart-tab-content').forEach(tab => {{
                        tab.classList.remove('active');
                    }});

                    // Remove active class from all buttons
                    document.querySelectorAll('.tab-button').forEach(btn => {{
                        btn.classList.remove('active');
                    }});

                    // Show selected tab
                    document.getElementById(tabName + '-tab').classList.add('active');
                    event.target.classList.add('active');
                }}
                </script>"""

            return charts_html

        except Exception as e:
            return f"""
            <div class="comprehensive-charts-error">
                <h3>Comprehensive Performance Charts</h3>
                <p>Could not load comprehensive charts: {e}</p>
            </div>"""

    def _plotly_config_to_html(self, chart_config: Dict[str, Any], chart_id: str) -> str:
        """Convert Plotly chart configuration to HTML."""
        try:
            import plotly.offline as py

            # Create figure from config
            fig = go.Figure(chart_config)

            # Generate HTML with inline JavaScript
            return fig.to_html(include_plotlyjs='inline', div_id=chart_id)
        except Exception as e:
            return f"<p>Error rendering chart: {e}</p>"

    def _create_memory_comparison_chart(self, raw_results: List[Dict[str, Any]]) -> str:
        """Create memory usage comparison chart."""
        if not PLOTLY_AVAILABLE:
            return "<p>Plotly not available for memory chart</p>"

        # Extract memory data from results
        memory_data = {}
        for result in raw_results:
            if not result or result.get('status') != 'ok':
                continue

            engine = result.get('engine_name', 'Unknown')
            memory_bytes = None

            # Try to find memory data
            for field in ['memory_peak_bytes', 'memory_compilation_bytes', 'memory_bytes']:
                if field in result and result[field] is not None:
                    memory_bytes = result[field]
                    break

            if memory_bytes and memory_bytes > 0:
                if engine not in memory_data:
                    memory_data[engine] = []
                memory_data[engine].append(memory_bytes / (1024 * 1024))  # Convert to MB

        if not memory_data:
            return "<p>No memory data available for chart</p>"

        try:
            # Create memory comparison chart
            engines = list(memory_data.keys())
            avg_memory = [sum(memory_data[engine])/len(memory_data[engine]) for engine in engines]
            max_memory = [max(memory_data[engine]) for engine in engines]

            fig = go.Figure(data=[
                go.Bar(name='Average Memory', x=engines, y=avg_memory,
                       marker_color='#3498db', opacity=0.8),
                go.Bar(name='Peak Memory', x=engines, y=max_memory,
                       marker_color='#e74c3c', opacity=0.6)
            ])

            fig.update_layout(
                title="Memory Usage Comparison (MB)",
                xaxis_title="Engine",
                yaxis_title="Memory Usage (MB)",
                barmode='group',
                template="plotly_white",
                height=400,
                showlegend=True
            )

            return fig.to_html(include_plotlyjs='inline', div_id="memory_comparison_chart")

        except Exception as e:
            return f"<p>Error creating memory chart: {e}</p>"

    def _create_throughput_comparison_chart(self, raw_results: List[Dict[str, Any]]) -> str:
        """Create interactive throughput comparison chart."""
        if not PLOTLY_AVAILABLE:
            return "<p>Plotly not available</p>"

        # Extract completed results with valid data
        valid_results = []
        for result in raw_results:
            if (result and result.get('status') == 'ok' and
                result.get('scanning_ns') and result.get('corpus_size_bytes')):
                # Calculate throughput in MB/s
                scanning_s = result['scanning_ns'] / 1_000_000_000
                corpus_mb = result['corpus_size_bytes'] / (1024 * 1024)
                if scanning_s > 0:
                    throughput = corpus_mb / scanning_s
                    valid_results.append({
                        'engine': result.get('engine_name', 'Unknown'),
                        'throughput': throughput,
                        'patterns': result.get('pattern_count', 0),
                        'corpus_mb': corpus_mb
                    })

        if len(valid_results) < 2:
            return "<p>Not enough valid results for throughput chart</p>"

        # Group by engine and calculate average throughput
        engines = {}
        for result in valid_results:
            engine = result['engine']
            if engine not in engines:
                engines[engine] = []
            engines[engine].append(result['throughput'])

        engine_names = list(engines.keys())
        avg_throughput = [sum(engines[engine])/len(engines[engine]) for engine in engine_names]
        max_throughput = [max(engines[engine]) for engine in engine_names]

        # Create bar chart
        fig = go.Figure(data=[
            go.Bar(name='Average Throughput', x=engine_names, y=avg_throughput,
                   marker_color='#3498db', opacity=0.8),
            go.Bar(name='Peak Throughput', x=engine_names, y=max_throughput,
                   marker_color='#2ecc71', opacity=0.6)
        ])

        fig.update_layout(
            title="Engine Throughput Comparison (MB/s)",
            xaxis_title="Engine",
            yaxis_title="Throughput (MB/s)",
            barmode='group',
            template="plotly_white",
            height=400,
            showlegend=True
        )

        return fig.to_html(include_plotlyjs='inline', div_id="throughput_chart")

    def _create_timing_distribution_chart(self, raw_results: List[Dict[str, Any]]) -> str:
        """Create timing distribution chart showing compilation vs scanning times."""
        if not PLOTLY_AVAILABLE:
            return "<p>Plotly not available</p>"

        # Extract timing data
        timing_data = []
        for result in raw_results:
            if (result and result.get('status') == 'ok' and
                result.get('compilation_ns') and result.get('scanning_ns')):
                timing_data.append({
                    'engine': result.get('engine_name', 'Unknown'),
                    'compilation_ms': result['compilation_ns'] / 1_000_000,
                    'scanning_ms': result['scanning_ns'] / 1_000_000,
                    'patterns': result.get('pattern_count', 0)
                })

        if len(timing_data) < 2:
            return "<p>Not enough timing data for distribution chart</p>"

        # Create stacked bar chart
        engines = {}
        for data in timing_data:
            engine = data['engine']
            if engine not in engines:
                engines[engine] = {'compilation': [], 'scanning': []}
            engines[engine]['compilation'].append(data['compilation_ms'])
            engines[engine]['scanning'].append(data['scanning_ms'])

        engine_names = list(engines.keys())
        avg_compilation = [sum(engines[engine]['compilation'])/len(engines[engine]['compilation'])
                          for engine in engine_names]
        avg_scanning = [sum(engines[engine]['scanning'])/len(engines[engine]['scanning'])
                       for engine in engine_names]

        fig = go.Figure(data=[
            go.Bar(name='Compilation Time', x=engine_names, y=avg_compilation,
                   marker_color='#e74c3c', opacity=0.8),
            go.Bar(name='Scanning Time', x=engine_names, y=avg_scanning,
                   marker_color='#f39c12', opacity=0.8)
        ])

        fig.update_layout(
            title="Average Timing Distribution (ms)",
            xaxis_title="Engine",
            yaxis_title="Time (ms)",
            barmode='stack',
            template="plotly_white",
            height=400,
            showlegend=True
        )

        return fig.to_html(include_plotlyjs='inline', div_id="timing_chart")

    def _create_engine_performance_chart(self, raw_results: List[Dict[str, Any]], analysis: Dict[str, Any]) -> str:
        """Create overall performance analysis chart."""
        if not PLOTLY_AVAILABLE:
            return "<p>Plotly not available</p>"

        # Extract performance metrics
        performance_data = []
        for result in raw_results:
            if (result and result.get('status') == 'ok' and
                result.get('scanning_ns') and result.get('corpus_size_bytes')):

                scanning_s = result['scanning_ns'] / 1_000_000_000
                corpus_mb = result['corpus_size_bytes'] / (1024 * 1024)
                throughput = corpus_mb / scanning_s if scanning_s > 0 else 0

                performance_data.append({
                    'engine': result.get('engine_name', 'Unknown'),
                    'throughput': throughput,
                    'patterns': result.get('pattern_count', 0),
                    'corpus_size': corpus_mb,
                    'match_count': result.get('match_count', 0)
                })

        if len(performance_data) < 2:
            return "<p>Not enough performance data for analysis chart</p>"

        # Create scatter plot showing throughput vs pattern count
        engines = {}
        for data in performance_data:
            engine = data['engine']
            if engine not in engines:
                engines[engine] = {'x': [], 'y': [], 'text': []}
            engines[engine]['x'].append(data['patterns'])
            engines[engine]['y'].append(data['throughput'])
            engines[engine]['text'].append(
                f"Corpus: {data['corpus_size']:.1f}MB<br>"
                f"Matches: {data['match_count']}"
            )

        fig = go.Figure()
        colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']

        for i, (engine, data) in enumerate(engines.items()):
            fig.add_trace(go.Scatter(
                x=data['x'],
                y=data['y'],
                mode='markers',
                name=engine,
                text=data['text'],
                hovertemplate=f'<b>{engine}</b><br>Patterns: %{{x}}<br>Throughput: %{{y:.2f}} MB/s<br>%{{text}}<extra></extra>',
                marker=dict(
                    size=10,
                    color=colors[i % len(colors)],
                    opacity=0.7
                )
            ))

        fig.update_layout(
            title="Performance Analysis: Throughput vs Pattern Count",
            xaxis_title="Pattern Count",
            yaxis_title="Throughput (MB/s)",
            template="plotly_white",
            height=400,
            showlegend=True
        )

        return fig.to_html(include_plotlyjs='inline', div_id="performance_chart")

    def _generate_raw_data_section(self, raw_results: List[Dict[str, Any]]) -> str:
        """Generate raw data section."""
        return f"""
        <section class="raw-data">
            <h2>üîç Raw Data</h2>
            <details>
                <summary>Click to expand raw benchmark data</summary>
                <pre class="raw-json">{json.dumps(raw_results, indent=2)}</pre>
            </details>
        </section>"""

    def _get_css_styles(self) -> str:
        """Get CSS styles for the HTML report."""
        return """
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f8f9fa;
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        header {
            text-align: center;
            margin-bottom: 30px;
            padding: 20px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        header h1 {
            margin: 0;
            color: #2c3e50;
        }
        .metadata {
            margin-top: 10px;
            color: #666;
            display: flex;
            justify-content: center;
            gap: 20px;
        }
        section {
            background: white;
            margin: 20px 0;
            padding: 25px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h2 {
            color: #2c3e50;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 10px;
        }
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }
        .summary-card {
            text-align: center;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 6px;
            border-left: 4px solid #3498db;
        }
        .card-label {
            font-size: 12px;
            text-transform: uppercase;
            color: #666;
            margin-bottom: 5px;
        }
        .card-value {
            font-size: 24px;
            font-weight: bold;
            color: #2c3e50;
        }
        .status-success {
            color: #27ae60 !important;
        }
        .status-error {
            color: #e74c3c !important;
        }
        .table-container {
            overflow-x: auto;
        }
        .results-table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 15px;
        }
        .results-table th,
        .results-table td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ecf0f1;
        }
        .results-table th {
            background: #f8f9fa;
            font-weight: 600;
            color: #2c3e50;
        }
        .results-table tr:hover {
            background: #f8f9fa;
        }
        .engine-name {
            font-weight: 600;
            color: #3498db;
        }
        .performance-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
        }
        .performance-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            border-left: 4px solid #9b59b6;
        }
        .throughput-summary {
            margin-top: 20px;
            padding: 20px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .throughput-summary h3 {
            margin: 0 0 15px 0;
            color: #2c3e50;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 10px;
        }
        .throughput-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin-top: 15px;
        }
        .throughput-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            border-left: 4px solid #e74c3c;
        }
        .throughput-card h4 {
            margin: 0 0 15px 0;
            color: #2c3e50;
            font-weight: 600;
            text-align: center;
        }
        .throughput-stats {
            display: flex;
            flex-direction: column;
            gap: 8px;
        }
        .stat-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 8px 12px;
            background: white;
            border-radius: 4px;
            border: 1px solid #ecf0f1;
        }
        .stat-label {
            color: #666;
            font-weight: 500;
        }
        .stat-value {
            color: #2c3e50;
            font-weight: 700;
            font-size: 14px;
        }
        .performance-card h4 {
            margin: 0 0 15px 0;
            color: #2c3e50;
        }
        .perf-item {
            display: flex;
            justify-content: space-between;
            margin: 8px 0;
        }
        .perf-item .label {
            color: #666;
        }
        .perf-item .value {
            font-weight: 600;
        }
        .chart-placeholder {
            text-align: center;
            padding: 40px;
            background: #f8f9fa;
            border-radius: 6px;
            color: #666;
        }
        .raw-json {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 4px;
            overflow-x: auto;
            font-size: 12px;
            white-space: pre-wrap;
        }
        footer {
            text-align: center;
            margin-top: 40px;
            color: #666;
            font-size: 14px;
        }
        footer a {
            color: #3498db;
            text-decoration: none;
        }

        /* Chart Styles */
        .charts {
            background: white;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .chart-grid {
            display: grid;
            grid-template-columns: 1fr;
            gap: 30px;
        }
        .chart-container {
            background: #fafafa;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            padding: 20px;
            margin: 10px 0;
        }
        .chart-container h3 {
            margin: 0 0 15px 0;
            color: #2c3e50;
            font-size: 1.2em;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        .chart-placeholder {
            text-align: center;
            padding: 40px;
            color: #666;
            background: #f8f9fa;
            border-radius: 8px;
            margin: 20px 0;
        }
        .chart-error {
            text-align: center;
            padding: 20px;
            color: #e74c3c;
            background: #fff5f5;
            border: 1px solid #fccfcf;
            border-radius: 8px;
            margin: 20px 0;
        }
        /* Memory Analysis Styles */
        .insights-section {
            margin: 30px 0;
            padding: 25px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #3498db;
        }
        .insight-cards {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }
        .insight-card {
            background: white;
            padding: 20px;
            border-radius: 6px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .insight-card h4 {
            margin: 0 0 10px 0;
            color: #2c3e50;
            font-size: 1.1em;
        }
        .insight-card p {
            margin: 0;
            color: #666;
            line-height: 1.5;
        }

        /* Memory Analysis Section */
        .memory-analysis-section {
            margin: 30px 0;
            padding: 25px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #9b59b6;
        }
        .memory-analysis-section h3 {
            margin: 0 0 20px 0;
            color: #2c3e50;
        }
        .memory-insights {
            margin-bottom: 25px;
            padding: 20px;
            background: white;
            border-radius: 6px;
        }
        .memory-insights h4 {
            margin: 0 0 15px 0;
            color: #2c3e50;
        }
        .insights-list {
            margin: 0;
            padding-left: 20px;
        }
        .insights-list li {
            margin: 8px 0;
            color: #555;
        }
        .memory-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
        }
        .memory-card {
            background: white;
            padding: 20px;
            border-radius: 6px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            border-left: 4px solid #9b59b6;
        }
        .memory-card h4 {
            margin: 0 0 15px 0;
            color: #2c3e50;
            text-align: center;
        }
        .memory-stats {
            display: flex;
            flex-direction: column;
            gap: 8px;
        }
        .memory-stat {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 8px 12px;
            background: #f8f9fa;
            border-radius: 4px;
        }
        .memory-stat .label {
            color: #666;
            font-weight: 500;
        }
        .memory-stat .value {
            color: #2c3e50;
            font-weight: 700;
        }
        .efficiency-highlight {
            background: linear-gradient(90deg, #e8f5e8, #f0f8f0) !important;
            border-left: 3px solid #27ae60 !important;
            font-weight: 600;
        }
        .efficiency-highlight .label {
            color: #27ae60 !important;
            font-weight: 600;
        }
        .efficiency-highlight .value {
            color: #2c3e50 !important;
            font-weight: 800;
            font-size: 1.1em;
        }

        /* Memory Overview Section */
        .memory-overview-section {
            margin: 30px 0;
            padding: 25px;
            background: #f8f9fa;
            border-radius: 8px;
            border-left: 4px solid #e74c3c;
        }
        .memory-overview-section h3 {
            margin: 0 0 20px 0;
            color: #2c3e50;
        }
        .memory-overview-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
        }
        .memory-overview-card {
            background: white;
            padding: 20px;
            border-radius: 6px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .memory-overview-card h4 {
            margin: 0 0 15px 0;
            color: #2c3e50;
            text-align: center;
        }
        .memory-overview-stats {
            display: flex;
            flex-direction: column;
            gap: 8px;
        }
        .memory-notice {
            margin: 30px 0;
            padding: 20px;
            background: #fff5f5;
            border: 1px solid #fccfcf;
            border-radius: 8px;
            text-align: center;
        }
        .memory-notice h3 {
            margin: 0 0 10px 0;
            color: #2c3e50;
        }
        .memory-analysis-error {
            margin: 30px 0;
            padding: 20px;
            background: #fff5f5;
            border: 1px solid #fccfcf;
            border-radius: 8px;
            text-align: center;
            color: #e74c3c;
        }

        /* Comprehensive Charts */
        .comprehensive-charts {
            margin: 30px 0;
            padding: 25px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .comprehensive-charts h3 {
            margin: 0 0 25px 0;
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        .chart-tabs {
            display: flex;
            margin-bottom: 25px;
            border-bottom: 1px solid #ecf0f1;
        }
        .tab-button {
            background: none;
            border: none;
            padding: 12px 20px;
            cursor: pointer;
            font-size: 14px;
            font-weight: 500;
            color: #666;
            border-bottom: 2px solid transparent;
            transition: all 0.3s ease;
        }
        .tab-button:hover {
            color: #3498db;
        }
        .tab-button.active {
            color: #3498db;
            border-bottom-color: #3498db;
        }
        .chart-tab-content {
            display: none;
        }
        .chart-tab-content.active {
            display: block;
        }
        .comprehensive-chart-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(500px, 1fr));
            gap: 30px;
        }
        .comprehensive-chart {
            background: #fafafa;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            padding: 20px;
        }
        .comprehensive-chart.full-width {
            grid-column: 1 / -1;
        }
        .comprehensive-chart h4 {
            margin: 0 0 15px 0;
            color: #2c3e50;
            font-size: 1.1em;
            border-bottom: 1px solid #e9ecef;
            padding-bottom: 10px;
        }
        .comprehensive-charts-error {
            margin: 30px 0;
            padding: 20px;
            background: #fff5f5;
            border: 1px solid #fccfcf;
            border-radius: 8px;
            text-align: center;
            color: #e74c3c;
        }

        /* Basic Chart Styles */
        .chart-section {
            margin: 20px 0;
        }
        .chart-section h3 {
            margin: 0 0 20px 0;
            color: #2c3e50;
            border-bottom: 2px solid #e74c3c;
            padding-bottom: 10px;
        }
        .basic-chart-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(450px, 1fr));
            gap: 20px;
        }
        .basic-chart-grid .chart-container h4 {
            margin: 0 0 15px 0;
            color: #2c3e50;
            font-size: 1em;
            border-bottom: 1px solid #e9ecef;
            padding-bottom: 8px;
        }

        /* Responsive chart grid for larger screens */
        @media (min-width: 1024px) {
            .chart-grid {
                grid-template-columns: 1fr 1fr;
            }
            .chart-container:last-child {
                grid-column: 1 / -1;
            }
        }

        @media (min-width: 1200px) {
            .comprehensive-chart-grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }
        """

    def _get_chart_scripts(self) -> str:
        """Get chart library scripts (placeholder)."""
        return """
        <!-- Chart.js would be included here -->
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        """

    def generate_enhanced_report(
        self,
        data: Dict[str, Any],
        output_dir: Path,
        include_charts: bool = False
    ) -> Path:
        """Generate an HTML report using enhanced database metadata for accurate job counts."""

        # Extract enhanced data structure
        metadata = data.get('metadata')
        benchmark_data = data.get('benchmark_results')

        # Create enhanced summary using database metadata for accurate job counts
        enhanced_summary = {}
        if metadata:
            enhanced_summary.update({
                'run_id': metadata.run_id,
                'total_combinations': metadata.total_jobs,  # Used by HTML formatter for Total Runs
                'successful_runs': metadata.completed_jobs,
                'failed_runs': metadata.failed_jobs,
                'queued': metadata.queued_jobs,
                'running': metadata.running_jobs,
                'skipped_lowvariance': metadata.total_jobs - metadata.completed_jobs - metadata.failed_jobs - metadata.queued_jobs - metadata.running_jobs,
                'status': metadata.status,
                'phase': 'production_fast_engines',  # Add phase info
                'created_at': metadata.created_at,
                'started_at': metadata.started_at,
                'completed_at': metadata.completed_at,
                'duration_seconds': metadata.duration_seconds if metadata.duration_seconds is not None else 0,
                'actual_execution_duration': metadata.actual_execution_duration if metadata.actual_execution_duration is not None else 'N/A',
                'config_path': metadata.config_path,
                'created_by': metadata.created_by,
            })

            # Add test matrix info if available
            if metadata.test_matrix:
                enhanced_summary['test_matrix'] = metadata.test_matrix

        # Merge with benchmark data (traditional or database-sourced)
        if benchmark_data:
            # Check if this is database-sourced data (has 'results' key) or traditional data (has 'raw_results' key)
            if 'results' in benchmark_data and 'raw_results' not in benchmark_data:
                # Database-sourced data - convert to expected format
                enhanced_data = {
                    'summary': {**benchmark_data.get('summary', {}), **enhanced_summary},
                    'raw_results': benchmark_data.get('results', []),
                    'engines': benchmark_data.get('engines', {}),
                    'analysis': {},
                    'metadata': {
                        'generated_at': data.get('generated_at'),
                        'database_path': data.get('database_path'),
                        'enhanced_reporting': True,
                        'data_source': 'database'
                    }
                }
            else:
                # Traditional data structure - use as-is but override job counts with database metadata
                enhanced_data = benchmark_data.copy()
                enhanced_data['summary'] = {**benchmark_data.get('summary', {}), **enhanced_summary}
                if 'metadata' not in enhanced_data:
                    enhanced_data['metadata'] = {}
                enhanced_data['metadata'].update({
                    'generated_at': data.get('generated_at'),
                    'database_path': data.get('database_path'),
                    'enhanced_reporting': True,
                    'data_source': 'traditional'
                })
        else:
            # Create minimal data structure from metadata only
            enhanced_data = {
                'summary': enhanced_summary,
                'raw_results': [],
                'analysis': {},
                'metadata': {
                    'generated_at': data.get('generated_at'),
                    'database_path': data.get('database_path'),
                    'enhanced_reporting': True,
                    'data_source': 'metadata_only'
                }
            }

        # Generate report using existing method with enhanced data
        return self.generate_report(enhanced_data, output_dir, include_charts)

    def _copy_assets(self, output_dir: Path) -> None:
        """Copy any additional assets needed for the report."""
        # Placeholder for copying CSS, JS, images, etc.
        pass


class MarkdownFormatter:
    """Generate Markdown reports."""

    def generate_report(
        self,
        data: Dict[str, Any],
        output_dir: Path,
        include_charts: bool = False
    ) -> Path:
        """Generate a Markdown report."""

        summary = data.get('summary', {})
        raw_results = data.get('raw_results', [])
        analysis = data.get('analysis', {})

        markdown = f"""# üöÄ Regex Benchmark Report

**Run ID:** {summary.get('run_id', 'Unknown')}
**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## üìä Summary

- **Phase:** {str(summary.get('phase', 'unknown')).title()}
- **Status:** {str(summary.get('status', 'unknown')).title()}
- **Duration:** {summary.get('duration_seconds', 0):.2f}s
- **Engines:** {', '.join(summary.get('engines_tested', []))}
- **Total Runs:** {summary.get('total_combinations', 0)}
- **Success Rate:** {(summary.get('successful_runs', 0) / max(summary.get('total_combinations', 1), 1) * 100):.1f}%

## üîß Results

| Engine | Status | Compilation (ms) | Scanning (ms) | Matches | Patterns | Regex/sec | Total Throughput |
|--------|--------|------------------|---------------|---------|----------|-----------|------------------|
"""

        for result in raw_results[:10]:  # Show first 10 results
            # Skip None results (incomplete jobs)
            if result is None:
                continue

            comp_ms = ((result.get('compilation_ns') or 0) / 1_000_000)
            scan_ms = ((result.get('scanning_ns') or 0) / 1_000_000)

            # Calculate pattern processing rate (regex/sec)
            patterns_per_sec_display = "-"
            patterns_compiled = result.get('patterns_compiled', 0)
            scanning_ns = result.get('scanning_ns', 0)
            if scanning_ns and scanning_ns > 0 and patterns_compiled and patterns_compiled > 0:
                scanning_seconds = scanning_ns / 1_000_000_000  # Convert nanoseconds to seconds
                patterns_per_sec = patterns_compiled / scanning_seconds
                if patterns_per_sec >= 1000:
                    patterns_per_sec_display = f"{patterns_per_sec/1000:.1f}K/sec"
                else:
                    patterns_per_sec_display = f"{patterns_per_sec:.1f}/sec"

            # Calculate total pattern throughput (patterns √ó MB/sec)
            total_throughput_display = "-"
            corpus_size_bytes = result.get('corpus_size_bytes', 0)
            corpus_size_mb = corpus_size_bytes / (1024 * 1024) if corpus_size_bytes and corpus_size_bytes > 0 else 0
            if scanning_ns and scanning_ns > 0 and patterns_compiled and patterns_compiled > 0 and corpus_size_mb > 0:
                scanning_seconds = scanning_ns / 1_000_000_000  # Convert nanoseconds to seconds
                total_throughput = (patterns_compiled * corpus_size_mb) / scanning_seconds
                if total_throughput >= 1000:
                    total_throughput_display = f"{total_throughput/1000:.1f}K p‚ãÖMB/s"
                else:
                    total_throughput_display = f"{total_throughput:.1f} p‚ãÖMB/s"

            markdown += f"| {result.get('engine_name', 'Unknown')} | {result.get('status', 'unknown')} | {comp_ms:.3f} | {scan_ms:.3f} | {result.get('match_count', 0)} | {result.get('patterns_compiled', 0)} | {patterns_per_sec_display} | {total_throughput_display} |\n"

        if len(raw_results) > 10:
            markdown += f"\n*... and {len(raw_results) - 10} more results*\n"

        markdown += f"""

## üìÑ Raw Data

```json
{json.dumps(summary, indent=2)}
```

---
*Generated by Regex Bench Framework v2.0*"""

        # Write markdown file
        report_file = output_dir / "benchmark_report.md"
        with open(report_file, 'w') as f:
            f.write(markdown)

        return report_file