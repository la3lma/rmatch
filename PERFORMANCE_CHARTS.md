# Performance Charts Documentation

This document explains the performance visualization system in rmatch, which automatically generates charts from both historical and recent performance test data.

## Overview

The performance charting system supports multiple data sources and generates different types of visualizations to track rmatch's performance evolution over time.

## Chart Types

### Historical Performance Charts (CSV-based)
- **Source**: `rmatch-tester/logs/*.csv`
- **Data Period**: Historical data from 2023
- **Charts Generated**:
  - `performance_evolution.png` - Comprehensive 4-panel view with execution time, memory usage, performance ratios, and test scale
  - `performance_trend.png` - Simplified chart for README inclusion
  - `simple_performance_evolution.png` - Basic time-series from individual test runs

### Recent Performance Test Charts (JSON-based)
- **Source**: `benchmarks/results/performance-check-*.json`
- **Data Period**: Recent automated performance tests
- **Charts Generated**:
  - `performance_check_evolution.png` - Detailed 4-panel view showing:
    - Execution time comparison (rmatch vs Java regex)
    - Memory usage comparison
    - Performance improvement percentages
    - Test status evolution (PASS/WARNING/FAIL)
  - `performance_check_overview.png` - Simplified scatter plot for recent results

### Combined Charts
- **`performance_summary.png`** - Two-panel view combining historical CSV data (top) with recent JSON data (bottom)

## Usage

### Generating Charts Manually
```bash
# Generate all charts from available data
python3 scripts/generate_performance_charts.py

# Generate sample performance data for demonstration
./scripts/generate_sample_performance_data.sh
```

### Automatic Generation
Charts are automatically generated by:
1. GitHub Actions workflow `.github/workflows/update-performance-charts.yml`
2. Triggered on pushes to main/master branch
3. Runs daily at 6 AM UTC
4. Also triggered manually via workflow_dispatch

### Adding New Performance Data

#### For CSV Data (Legacy)
Add files to `rmatch-tester/logs/` in the expected format:
- `large-corpus-log.csv` - Comprehensive comparison data
- `logfile-*.csv` - Individual test run data

#### For JSON Data (Recommended)
Run performance tests that generate JSON reports:
```bash
./mvnw -q -B -pl rmatch-tester exec:java \
  -Dexec.mainClass=no.rmz.rmatch.performancetests.GitHubActionPerformanceTestRunner \
  -Dexec.args="100 3"  # 100 regexps, 3 runs
```

This generates files in `benchmarks/results/performance-check-*.json` format.

## Data Format Details

### JSON Performance Check Format
```json
{
  "timestamp": "2025-09-07T13:52:40.250020663Z",
  "performance_result": {
    "status": "PASS",
    "explanation": "Performance improvement: time 3.1%, memory 87.2%",
    "time_improvement_percent": 0.031,
    "memory_improvement_percent": 0.872,
    "statistically_significant": false
  },
  "current_results": {
    "rmatch": {
      "count": 3,
      "avg_time_ms": 393.33,
      "avg_memory_mb": 89.33
    },
    "java": {
      "count": 3,
      "avg_time_ms": 86.0,
      "avg_memory_mb": 90.33
    }
  }
}
```

### CSV Legacy Format
- `large-corpus-log.csv`: timestamp, testSeriesId, metadata, matcherTypeName1, usedMemoryInMb1, durationInMillis1, matcherTypeName2, usedMemoryInMb2, durationInMillis2, ...
- `logfile-*.csv`: NoOfRegexps, javaMillis, regexMillis, quotient

## Chart Interpretation

### Time Axis Formatting
Performance charts use different time axis formats depending on the data type:

- **Historical Charts (CSV data)**: Display year-month format (e.g., "2023-05") suitable for long-term trends
- **Recent Performance Charts (JSON data)**: Display year-month-day hour:minute format (e.g., "2025-01-24 15:30") providing precise timing with full date context for recent test results

This dual formatting approach ensures historical data remains readable across long time periods while recent performance data shows the precise timing with full year context needed to correlate performance changes with specific test runs.

### Status Colors in Recent Performance Charts
- ðŸŸ¢ **Green**: PASS - Performance meets criteria
- ðŸŸ  **Orange**: WARNING - Performance within noise threshold
- ðŸ”´ **Red**: FAIL - Performance regression detected

### Performance Ratios
- **< 1.0**: rmatch is faster than Java regex (better)
- **= 1.0**: Equal performance (parity line)
- **> 1.0**: rmatch is slower than Java regex (worse)

## Customization

### Adding New Chart Types
1. Create a new function in `scripts/generate_performance_charts.py`
2. Follow the pattern of existing chart functions
3. Call the function from `main()`
4. Update documentation

### Modifying Chart Appearance
Charts use matplotlib with seaborn styling. Key customization points:
- Color schemes: Modify the color definitions in chart functions
- Chart size: Adjust `figsize` parameters
- Data filtering: Modify data selection logic for time ranges
- Styling: Update matplotlib styling options

## Troubleshooting

### No Charts Generated
- Check that data sources exist (`rmatch-tester/logs/*.csv` or `benchmarks/results/*.json`)
- Verify Python dependencies are installed: `pip install pandas matplotlib seaborn numpy`
- Check for error messages in the script output

### Charts Look Empty
- Ensure timestamp fields are valid and parseable
- Check that required columns exist in the data
- Verify data types match expected formats

### Performance Test Failures
- Performance tests may "fail" if rmatch is slower than baseline - this is expected behavior
- The test status is separate from chart generation success
- Charts will still be generated even if tests fail performance criteria