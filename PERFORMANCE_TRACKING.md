# Performance Charts Documentation

This document explains the modern performance visualization system in rmatch, which generates comprehensive charts from benchmark data in `benchmarks/results/`.

## Overview

The performance tracking system is completely redesigned to be:
- **Clean & Focused**: Based solely on data from `benchmarks/results/`
- **Comprehensive**: Covers both micro and macro benchmark results
- **Automated**: Charts auto-generated via GitHub Actions
- **Modern**: Clean, professional visualization style

## Data Sources

All performance data comes from `benchmarks/results/`:

### JMH Benchmark Results (`jmh-*.json`)
- **Purpose**: Precise microbenchmark measurements
- **Metrics**: Execution time, statistical confidence intervals, percentiles
- **Format**: Standard JMH JSON output with detailed performance statistics
- **Example**: `jmh-20250907T121845Z.json`

### Macro Benchmark Results (`macro-*.json`) 
- **Purpose**: End-to-end performance testing with realistic workloads
- **Metrics**: Total execution time, exit status, configuration parameters
- **Format**: Custom JSON format with metadata and timing results
- **Example**: `macro-20250907T194234Z.json`

## Generated Charts

### 1. Performance Overview (`performance_overview.png`)
- **Layout**: 2x2 grid showing key metrics at a glance
- **Content**: 
  - JMH performance trends
  - Macro benchmark duration
  - Performance statistics summary
  - Benchmark data summary
- **Use**: Main chart for README and documentation

### 2. JMH Performance Evolution (`jmh_performance_evolution.png`)
- **Layout**: Detailed dual-panel analysis
- **Content**:
  - Performance trends over time (by pattern count if available)
  - Performance distribution analysis or recent results
- **Use**: Detailed JMH performance analysis

### 3. Macro Performance Evolution (`macro_performance_evolution.png`)
- **Layout**: Dual-panel macro benchmark analysis  
- **Content**:
  - Execution time trends over time
  - Performance vs. pattern count analysis
- **Use**: Macro benchmark performance tracking
- **Note**: Generated only when macro data is available

## Chart Generation

### Automatic Generation
Charts are automatically generated by:
```bash
python3 scripts/generate_benchmarks_charts.py
```

### Manual Generation
```bash
cd /path/to/rmatch
python3 scripts/generate_benchmarks_charts.py
```

### Dependencies
- Python 3.8+
- pandas >= 1.5.0
- matplotlib >= 3.5.0  
- seaborn >= 0.11.0
- numpy >= 1.21.0

## Chart Styling

- **Style**: Modern, clean seaborn-based styling
- **Colors**: Professional color palette with high contrast
- **Resolution**: High-DPI (300 DPI) for crisp display
- **Format**: PNG with optimized compression

## Integration

### README.md Integration
The performance section in README.md shows:
1. Overview chart for quick performance assessment
2. Detailed JMH evolution for in-depth analysis
3. Key metrics summary with data source information

### GitHub Actions Integration
Charts are automatically regenerated when:
- New benchmark results are added to `benchmarks/results/`
- Performance testing workflows complete
- Manual workflow triggers are activated

## Data Format Examples

### JMH Result Structure
```json
{
  "jmhVersion": "1.37",
  "benchmark": "no.rmz.rmatch.benchmarks.CompileAndMatchBench.buildMatcher",
  "mode": "avgt",
  "primaryMetric": {
    "score": 2.338449118055604,
    "scoreUnit": "us/op",
    "scoreError": "NaN"
  },
  "params": {
    "patternCount": "1"
  }
}
```

### Macro Result Structure
```json
{
  "type": "macro",
  "timestamp": "20250907T194234Z",
  "duration_ms": 17784,
  "exit_status": 0,
  "args": { "max_regexps": 10000 }
}
```

## Migration from Legacy System

The previous performance tracking system used multiple data sources (CSV files, mixed JSON formats). The new system:

- ✅ **Simplified**: Single data source (`benchmarks/results/`)
- ✅ **Reliable**: Consistent data format and processing
- ✅ **Maintainable**: Single script for all chart generation
- ✅ **Professional**: Clean, modern chart styling
- ✅ **Comprehensive**: Covers all benchmark types

## Troubleshooting

### No Charts Generated
1. Verify benchmark data exists in `benchmarks/results/`
2. Check Python dependencies are installed
3. Run script with verbose output: `python3 -v scripts/generate_benchmarks_charts.py`

### Chart Quality Issues
- Charts are generated at 300 DPI for high quality
- Use PNG format for optimal display in documentation
- Ensure matplotlib backend supports PNG output

### Data Format Issues
- JMH data should be valid JSON from JMH output
- Macro data should follow the expected JSON structure
- Malformed JSON files are logged and skipped