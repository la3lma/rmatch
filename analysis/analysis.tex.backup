\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{url}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage[font=it]{caption}

\geometry{margin=2.5cm}
\pagestyle{fancy}
\fancyhf{}
\rhead{rmatch Performance Analysis}
\lhead{\thepage}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    breaklines=true,
    showstringspaces=false,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\title{rmatch: Performance Analysis and Optimization Roadmap}
\author{Technical Analysis Report}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report provides a comprehensive analysis of the rmatch regular expression matching library, identifying key performance bottlenecks and proposing optimization strategies to achieve more than 10x performance improvement. We analyze the current Thompson NFA + subset construction implementation, identify critical algorithmic and implementation issues, and propose a roadmap incorporating modern regex matching techniques including Aho-Corasick pattern matching, bit-parallel operations, and SIMD optimizations.
\end{abstract}

\section{Executive Summary}

The rmatch library implements a classic Thompson NFA construction approach with on-the-fly DFA generation via subset construction. Since the original analysis, major algorithmic optimizations have been implemented, including first-character optimization (fixing the critical O(m×l) bottleneck) and Aho-Corasick prefiltering. However, performance remains limited to approximately 10\% of Java's standard regex matcher due to several key optimizations being disabled by default and remaining data structure inefficiencies.

\subsection{Key Findings:}

\begin{itemize}
\item \textbf{RESOLVED:} Critical O(m×l) complexity bottleneck has been fixed with first-character optimization, now O(m×k) where k is the average number of patterns that can start with each character
\item \textbf{IMPLEMENTED:} Aho-Corasick prefilter algorithm for literal pattern acceleration (disabled by default)
\item \textbf{PENDING:} Data structure inefficiencies with excessive synchronization overhead remain
\item \textbf{CRITICAL FINDING:} Major optimizations implemented but not enabled by default, limiting realized performance gains to only a few percent rather than expected 10-50x improvements
\end{itemize}

\section{Current Implementation Analysis}

\subsection{Architecture Overview}

The rmatch system consists of several key components working together to provide multi-pattern regex matching:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{illustrations/current-architecture.png}
\caption{Current rmatch Architecture}
\label{fig:current-arch}
\end{figure}

\subsubsection{Core Components}

\begin{itemize}

\item \textbf{ARegexpCompiler:} Implements Thompson NFA construction~\cite{thompson1968programming}. Converts regular expression strings into non-deterministic finite automata using standard recursive descent parsing.

\item \textbf{NodeStorageImpl:} Manages the subset construction algorithm~\cite{hopcroft2001introduction} for converting NFA states to DFA states on-demand. Uses synchronized maps to cache previously computed state transitions.

\item \textbf{MatchEngineImpl:} The main matching engine that processes input text character by character, maintaining active match sets and progressing through the automaton.

\item \textbf{MatchSetImpl:} Represents a collection of potential matches starting from the same input position. This is where the most critical performance bottleneck occurs.


\end{itemize}
  
\subsection{Critical Performance Bottleneck Analysis}

\subsubsection{RESOLVED: The O(m×l) Complexity Problem}

\textbf{Status: FIXED via First-Character Optimization}

The most severe performance issue that previously existed in the \texttt{MatchSetImpl} constructor has been successfully addressed. The original implementation, explicitly identified in the code comments as "the most egregious bug in the whole regexp package", created a match object for every regular expression at every starting position:

\begin{lstlisting}[language=Java,caption=Original critical bottleneck (now fixed)]
// ORIGINAL CODE (now optimized):
// XXX This lines represents the most egregious
//     bug in the whole regexp package, since it
//     incurs a cost in both runtime and used memory
//     directly proportional to the number of
//     expressions (m) the matcher matches for.  For a
//     text that is l characters long, this  in turns
//     adds a factor O(l*m) to the resource use of the
//     algorithm.

for (final Regexp r : this.currentNode.getRegexps()) {
    matches.add(this.currentNode.newMatch(this, r));  // O(m×l) complexity
}
\end{lstlisting}

\textbf{Solution Implemented:} First-character optimization now filters patterns based on the current character being processed, reducing complexity from O(m×l) to O(m×k) where k is the average number of patterns that can start with each character (typically k << m).

\subsubsection{Data Structure Inefficiencies}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{illustrations/data-structure-overhead.png}
\caption{Current Data Structure Overhead}
\label{fig:data-overhead}
\end{figure}

The implementation suffers from several data structure inefficiencies:

\begin{itemize}
\item \textbf{Excessive Synchronization:} Heavy use of \texttt{ConcurrentHashMap}, \texttt{Collections.synchronizedSet()}, and manual synchronization blocks
\item \textbf{Object Allocation Overhead:} Constant creation of \texttt{Match}, \texttt{MatchSet}, and intermediate collection objects
\item \textbf{Inefficient State Representation:} DFA states represented as heavyweight \texttt{SortedSet<NDFANode>} objects
\item \textbf{String-based Counters:} Performance monitoring using string-keyed synchronized counters
\end{itemize}

\subsubsection{Algorithmic Limitations}

The current implementation lacks several critical optimizations found in modern regex engines:

\begin{itemize}
\item \textbf{No First-Character Optimization:} Every pattern is considered at every position
\item \textbf{No Boyer-Moore Skip Tables:} Cannot skip characters that don't appear in patterns
\item \textbf{No Bit-Parallel Operations:} Sequential character-by-character processing only
\item \textbf{No State Minimization:} DFA states are not minimized, leading to state explosion
\item \textbf{No Prefix/Suffix Sharing:} Common pattern elements not factored out
\end{itemize}

\section{Literature Review and Modern Techniques}

\subsection{Aho-Corasick Algorithm}

The Aho-Corasick algorithm~\cite{aho1975efficient} provides optimal O(n+m+z) time complexity for finding all occurrences of multiple string patterns, where n is text length, m is total pattern length, and z is the number of matches.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{illustrations/aho-corasick-comparison.png}
\caption{Aho-Corasick vs Current Approach}
\label{fig:aho-corasick}
\end{figure}

\textbf{Key Benefits for rmatch:}
\begin{itemize}
\item Eliminates the O(m) overhead per character for literal string patterns
\item Provides optimal failure function for pattern matching
\item Can be extended to handle regex constructs via hybrid approaches
\end{itemize}

\subsection{Bit-Parallel Regex Matching}

Bit-parallel approaches~\cite{baeza1992new,myers1999fast} use bitwise operations to simulate NFAs efficiently:

\begin{algorithm}
\caption{Bit-Parallel NFA Simulation}
\begin{algorithmic}[1]
\State $D_0 \gets$ initial state bitvector
\For{each character $c$ in text}
    \State $D_{i+1} \gets (D_i \ll 1) \land T[c]$
    \If{$D_{i+1} \land F \neq 0$}
        \State report match
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

Where $T[c]$ is a precomputed transition table for character $c$, and $F$ is the final state bitvector.

\subsection{SIMD and Vectorization Techniques}

Modern regex engines like Hyperscan~\cite{intel2016hyperscan} leverage SIMD instructions for massive parallelization:

\begin{itemize}
\item \textbf{Character Class Matching:} Use SIMD to test multiple characters against character classes simultaneously
\item \textbf{Parallel State Simulation:} Run multiple automata states in parallel using vector operations
\item \textbf{String Scanning:} Use SIMD string scanning primitives for literal pattern detection
\end{itemize}

\subsection{RE2-Style Optimizations}

Google's RE2 engine~\cite{cox2007regular} demonstrates several key optimizations:

\begin{itemize}
\item \textbf{Lazy DFA Construction:} Build DFA states only when needed during matching
\item \textbf{State Caching:} Intelligently cache and reuse computed states
\item \textbf{Literal Extraction:} Extract literal prefixes/suffixes for fast filtering
\item \textbf{One-Pass Construction:} Optimize for common single-pass regex patterns
\end{itemize}

\section{Proposed Optimization Strategy}

\subsection{Phase 1: Eliminate Critical Bottlenecks (Expected 3-5x improvement)}

\subsubsection{COMPLETED: Fix O(m×l) Complexity}

\textbf{Status: IMPLEMENTED AND ACTIVE}

First-character heuristics have been successfully implemented to eliminate the critical bottleneck:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{illustrations/first-char-optimization.png}
\caption{First-Character Optimization Strategy (IMPLEMENTED)}
\label{fig:first-char}
\end{figure}

\begin{lstlisting}[language=Java,caption=Implemented first-character optimization]
// IMPLEMENTED: MatchSetImpl constructor with character optimization
public MatchSetImpl(final int startIndex, final DFANode newCurrentNode, 
                   final Character currentChar) {
    // Use first-character heuristic to filter regexps
    final Set<Regexp> candidateRegexps;
    if (currentChar != null) {
        candidateRegexps = this.currentNode.getRegexpsThatCanStartWith(currentChar);
    } else {
        candidateRegexps = this.currentNode.getRegexps();
    }
    
    // Early exit if no regexps can match
    if (candidateRegexps.isEmpty()) {
        this.matches = ConcurrentHashMap.newKeySet(0);
        return;
    }
    
    // Only create matches for viable patterns
    for (final Regexp r : candidateRegexps) {
        matches.add(this.currentNode.newMatch(this, r));
    }
}
\end{lstlisting}

\textbf{Implementation Details:}
\begin{itemize}
\item \texttt{DFANodeImpl.getRegexpsThatCanStartWith(Character ch)} with caching
\item \texttt{RegexpImpl.canStartWith(Character ch)} with first-character analysis  
\item Active integration in \texttt{MatchEngineImpl.matcherProgress()}
\item Comprehensive test coverage in \texttt{FirstCharacterOptimizationTest}
\end{itemize}

\subsubsection{Replace Heavyweight Data Structures}

\begin{itemize}
\item \textbf{Replace \texttt{SortedSet<NDFANode>} with compact \texttt{int[]} arrays:} The current implementation uses heavyweight \texttt{SortedSet<NDFANode>} objects to represent DFA states, which incurs significant memory overhead and requires expensive set operations for comparisons. By mapping each \texttt{NDFANode} to a unique integer ID, we can represent state sets as compact integer arrays or bitsets, reducing memory usage by 80-90\% and enabling faster set operations through bitwise arithmetic.

\item \textbf{Use lock-free data structures for multi-threading:} The pervasive use of \texttt{ConcurrentHashMap}, \texttt{Collections.synchronizedSet()}, and manual synchronization blocks creates lock contention and limits scalability. Implementing lock-free alternatives using atomic operations and compare-and-swap techniques will eliminate blocking, reduce context switching overhead, and improve throughput in multi-threaded scenarios by 3-5x.

\item \textbf{Implement object pooling for frequently allocated objects:} The constant creation and destruction of \texttt{Match}, \texttt{MatchSet}, and intermediate collection objects generates excessive garbage collection pressure, particularly problematic for the O(m×l) bottleneck. By implementing object pools that reuse these frequently allocated objects, we can reduce GC overhead by 60-80\% and improve cache locality through better memory access patterns.

\item \textbf{Replace string-based counters with primitive arrays:} The current performance monitoring system uses string-keyed synchronized maps for counters, adding unnecessary overhead to every operation. Replacing these with simple primitive arrays indexed by operation type will eliminate string hashing, reduce synchronization overhead, and provide microsecond-level performance metrics without impacting the core matching performance.
\end{itemize}

\subsection{Phase 2: Algorithmic Enhancements (Expected 2-3x improvement)}

\subsubsection{COMPLETED: Hybrid Aho-Corasick Integration}

\textbf{Status: IMPLEMENTED (requires manual activation)}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{illustrations/hybrid-approach.png}
\caption{Hybrid Aho-Corasick + NFA Architecture (IMPLEMENTED)}
\label{fig:hybrid}
\end{figure}

The two-tier approach has been successfully implemented:
\begin{enumerate}
\item \textbf{Completed:} Aho-Corasick algorithm for literal pattern prefixes
\item \textbf{Completed:} Fallback to NFA simulation when necessary
\item \textbf{Pending:} Shared common prefixes and suffixes across patterns
\end{enumerate}

\subsubsection{IMPLEMENTED: Aho-Corasick Literal Prefilter}

\textbf{Status:} Fully implemented and tested, but disabled by default (requires \texttt{-Drmatch.prefilter=aho}).

A comprehensive literal substring prefilter has been successfully integrated into the rmatch engine, providing substantial performance improvements for pattern sets containing extractable literals. This implementation represents a major milestone in the optimization roadmap.

\textbf{Architecture Overview:}

The prefilter operates in two phases:
\begin{enumerate}
\item \textbf{Compile-time literal extraction:} The \texttt{LiteralPrefilter} class analyzes regex patterns to extract the longest literal substring from each pattern, handling quoted sections (\texttt{\textbackslash Q...\textbackslash E}), escaped characters, and complex regex constructs.
\item \textbf{Runtime Aho-Corasick scanning:} The \texttt{AhoCorasickPrefilter} class builds an AC automaton from extracted literals and scans input text once to identify candidate positions where regex matching should be attempted.
\end{enumerate}

\textbf{Key Components:}

\begin{itemize}
\item \texttt{LiteralHint}: Model class representing extracted literal information including pattern ID, literal string, anchoring information, and case sensitivity flags.

\item \texttt{LiteralPrefilter}: Compile-time analyzer that extracts literal substrings using a state machine approach, handling:
  \begin{itemize}
  \item Quoted literal sections (\texttt{\textbackslash Q...\textbackslash E})
  \item Escaped characters and metacharacters  
  \item Non-capturing groups (\texttt{(?:...)})
  \item Character classes and alternations
  \end{itemize}

\item \texttt{AhoCorasickPrefilter}: Runtime component that constructs and operates an AC automaton, supporting case-insensitive matching and multiple patterns sharing identical literals.

\item \texttt{MatchEngineImpl}: Enhanced with prefilter integration, controlled by the system property \texttt{rmatch.prefilter=aho}.
\end{itemize}

\textbf{Performance Characteristics:}

The prefilter reduces the O(m×l) bottleneck by identifying sparse candidate positions where regex matching should occur, rather than testing every pattern at every position. Expected performance improvements:

\begin{itemize}
\item \textbf{Large pattern sets (100+ patterns):} 5-15x improvement when patterns contain extractable literals
\item \textbf{Long input texts:} Linear scaling with text length rather than quadratic 
\item \textbf{Patterns with common prefixes:} Shared literals reduce redundant scanning
\end{itemize}

\textbf{Usage:}

\begin{lstlisting}[language=bash,caption=Enabling the prefilter]
# Enable Aho-Corasick prefilter
java -Drmatch.prefilter=aho MyApplication

# Disable prefilter (default behavior) 
java -Drmatch.prefilter=off MyApplication
\end{lstlisting}

\textbf{Limitations and Current Issues:}

\begin{itemize}
\item \textbf{CRITICAL ISSUE:} Prefilter is disabled by default, requiring manual configuration via \texttt{-Drmatch.prefilter=aho}
\item \textbf{Performance Impact:} This explains why implemented optimizations provide "only a few percent" improvement rather than expected 10-50x gains
\item Patterns with no extractable literals (e.g., \texttt{.*}, \texttt{[a-z]+}) fall back to original O(m×l) behavior
\item Currently requires manual prefilter configuration; automatic integration with pattern compilation is planned
\item Case-insensitive handling uses simple upper/lower expansion; full Unicode normalization could improve coverage
\end{itemize}

\textbf{Recommendation:} Enable prefilter by default for patterns with extractable literals to realize the full performance benefits.

\subsubsection{Bit-Parallel NFA Simulation}

For patterns with up to 64 states, implement bit-parallel simulation:
\begin{itemize}
\item Represent NFA states as 64-bit integers
\item Use bitwise operations for state transitions
\item Leverage CPU's parallel bit manipulation instructions
\end{itemize}

\subsection{Phase 3: Advanced Optimizations (Expected 2-4x improvement)}

\subsubsection{SIMD Integration}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{illustrations/simd-optimization.png}
\caption{SIMD Character Processing}
\label{fig:simd}
\end{figure}

Leverage Java's Vector API (JEP 338) for SIMD operations:
\begin{itemize}
\item Process 16-32 characters simultaneously for character class matching
\item Implement SIMD-based string scanning for literal patterns
\item Use vectorized comparison operations for multiple pattern matching
\end{itemize}

\subsubsection{Advanced State Management}

\begin{itemize}
\item Implement DFA state minimization to reduce memory usage
\item Add intelligent state caching strategies
\item Use compressed state representations
\item Implement state garbage collection for long-running matches
\end{itemize}

\section{Implementation Roadmap}

\subsection{Development Phases}

\begin{table}[htbp]
\centering
\begin{tabular}{@{}lllr@{}}
\toprule
\textbf{Phase} & \textbf{Duration} & \textbf{Key Deliverables} & \textbf{Expected Gain} \\
\midrule
Phase 1 & 2-3 weeks & First-char optimization, data structure replacement & 3-5x \\
Phase 2 & 3-4 weeks & Aho-Corasick integration, bit-parallel simulation & 2-3x \\
Phase 3 & 4-6 weeks & SIMD operations, advanced state management & 2-4x \\
\midrule
\textbf{Total} & \textbf{9-13 weeks} & \textbf{Complete optimization} & \textbf{12-60x} \\
\bottomrule
\end{tabular}
\caption{Implementation Timeline and Expected Performance Gains}
\label{tab:roadmap}
\end{table}

\subsection{Risk Mitigation}

\begin{itemize}
\item Maintain backward API compatibility throughout all phases
\item Implement comprehensive benchmarking suite for regression detection
\item Use feature flags for gradual rollout of optimizations
\item Maintain fallback to current implementation for edge cases
\end{itemize}

\section{Benchmarking and Validation}

\subsection{Performance Testing Strategy}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{illustrations/benchmark-strategy.png}
\caption{Comprehensive Benchmarking Strategy}
\label{fig:benchmark}
\end{figure}

\subsubsection{Test Scenarios:}
\begin{itemize}
\item Small pattern sets (1-10 patterns) with various text sizes
\item Medium pattern sets (10-100 patterns) with realistic corpus data  
\item Large pattern sets (100-10,000 patterns) with streaming data
\item Complex patterns with quantifiers, character classes, and alternations
\item Real-world patterns from log processing, genomics, and text mining
\end{itemize}

\subsubsection{Metrics to Track:}
\begin{itemize}
\item Throughput (MB/s processed)
\item Latency percentiles (p50, p95, p99)
\item Memory allocation rates
\item CPU utilization and cache hit rates
\item Scalability with increasing pattern counts
\end{itemize}

\subsection{Validation Against Reference Implementations}

Compare performance against established regex engines:
\begin{itemize}
\item \textbf{Java's standard \texttt{java.util.regex} package:} Implement direct comparison tests using identical pattern sets and test data, measuring both single-pattern performance via \texttt{Pattern.matcher()} and multi-pattern scenarios using multiple \texttt{Pattern} instances. Create JMH benchmarks that isolate compilation time from matching time, and compare memory allocation patterns using JVM profiling tools like JProfiler or async-profiler to identify where rmatch's object creation overhead becomes significant.

\item \textbf{Google's RE2 engine (via JNI bindings):} Integrate RE2/J or similar JNI wrapper to enable direct performance comparisons with RE2's linear-time guarantees. Design test suites that specifically target RE2's strengths (complex patterns with potential exponential backtracking) and weaknesses (simple literal matching) to understand the performance trade-offs. Measure JNI call overhead separately to isolate pure algorithmic performance differences.

\item \textbf{PCRE library performance characteristics:} Use PCRE4J or similar bindings to compare against PCRE's optimized backtracking engine, focusing on patterns where backtracking engines excel (complex lookaheads, backreferences). Document cases where rmatch's NFA approach provides better worst-case guarantees than PCRE's potentially exponential behavior, and quantify the performance differences across pattern complexity spectrums.

\item \textbf{Specialized multi-pattern matchers like Hyperscan:} Establish baseline comparisons with Intel's Hyperscan library for scenarios involving hundreds to thousands of patterns, which represents rmatch's primary use case. Use Hyperscan's streaming API to compare against rmatch's buffer-based matching, measuring both throughput and memory usage. Focus on identifying the pattern count threshold where specialized multi-pattern engines begin to outperform general-purpose regex libraries significantly.
\end{itemize}

\section{Critical Analysis: Why Performance Improvements Are Minimal}

\subsection{Current State vs Expected Performance}

Despite implementing key optimizations (first-character filtering and Aho-Corasick prefilter), performance improvements remain "only a few percent" rather than the expected 3-5x to 10x gains. Analysis reveals the following critical issues:

\subsubsection{Root Cause Analysis}

\begin{itemize}
\item \textbf{Prefilter Disabled by Default:} The Aho-Corasick prefilter, which should provide 10-50x improvement for literal patterns, is disabled by default and requires manual activation via \texttt{-Drmatch.prefilter=aho}

\item \textbf{Integration Gaps:} While first-character optimization is active, its benefits may be limited by:
  \begin{itemize}
  \item Continued character-by-character processing for complex patterns
  \item Lack of pattern analysis to enable prefilter automatically
  \item Missing optimization for patterns without clear first characters
  \end{itemize}

\item \textbf{Remaining Data Structure Inefficiencies:} Heavy synchronization overhead and object allocation patterns persist:
  \begin{itemize}
  \item \texttt{ConcurrentHashMap} and synchronized collections still used extensively
  \item Object pooling not implemented for \texttt{Match}/\texttt{MatchSet} objects
  \item String-based performance counters add overhead
  \end{itemize}
\end{itemize}

\subsection{Recommended Next Steps}

\subsubsection{Immediate Actions (Expected 10-30x improvement)}

\begin{enumerate}
\item \textbf{Enable prefilter by default} for patterns with extractable literals
\item \textbf{Automatic literal extraction} during pattern compilation
\item \textbf{Hybrid matching strategy} that seamlessly combines prefiltering with NFA simulation
\end{enumerate}

\subsubsection{Phase 1 Remaining Items (Expected 2-4x improvement)}

\begin{enumerate}
\item Replace \texttt{ConcurrentHashMap} with lock-free alternatives
\item Implement object pooling for frequently allocated objects  
\item Replace string-based counters with primitive arrays
\item Use compact \texttt{int[]} arrays instead of \texttt{SortedSet<NDFANode>}
\end{enumerate}

\subsubsection{New Optimization Targets}

Based on current analysis, additional opportunities include:

\begin{itemize}
\item \textbf{Pattern Compilation Optimization:} Analyze patterns at compile time to choose optimal matching strategy
\item \textbf{Adaptive Algorithm Selection:} Use different algorithms based on pattern characteristics
\item \textbf{State Machine Optimization:} Minimize DFA states and optimize transitions
\item \textbf{Memory Layout Optimization:} Improve cache locality and reduce memory fragmentation
\end{itemize}

\section{Conclusion}

The rmatch library has undergone significant optimization since the original analysis, with major algorithmic improvements successfully implemented. The critical O(m×l) complexity bottleneck has been resolved through first-character optimization, transforming the complexity to O(m×k) where k represents the average number of patterns that can start with each character. Additionally, a comprehensive Aho-Corasick prefilter system has been implemented and integrated.

\textbf{Current Status Summary:}
\begin{itemize}
\item \textbf{Resolved:} Critical O(m×l) bottleneck through first-character optimization
\item \textbf{Implemented:} Full Aho-Corasick prefilter system with literal pattern extraction
\item \textbf{Critical Issue:} Major optimizations disabled by default, limiting performance gains to <5\% rather than expected 10-50x
\item \textbf{Remaining:} Data structure inefficiencies and synchronization overhead
\end{itemize}

\textbf{Key Insight:} The bottleneck is no longer missing algorithmic optimizations but rather configuration and integration issues that prevent the realization of implemented improvements. The immediate opportunity lies in enabling the prefilter by default and optimizing the integration between different optimization strategies.

By addressing the remaining configuration issues and completing the data structure optimizations, rmatch has the potential to achieve 15-40x performance improvements over the baseline, positioning it as a highly competitive multi-pattern regex matching library. The foundation has been laid; the focus now shifts from algorithmic development to optimization activation and integration refinement.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
