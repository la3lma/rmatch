#!/usr/bin/env python3
"""
Generate performance charts from benchmarks/results data only.
Updated version addressing GitHub issue requirements for improved JMH performance evolution charts.

Key improvements:
- Timestamps include year (YYYY-MM-DD HH:MM format)
- Focus on 5000 and 10000 pattern counts as most important
- Memory usage visualization (when data is available)
- rmatch vs Java comparison charts and ratios
- Removed performance distribution chart as requested
"""

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
import numpy as np
import json
import os
import re
from datetime import datetime
from pathlib import Path

# Configure matplotlib for better-looking plots
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

def load_jmh_data(results_dir="benchmarks/results"):
    """Load and parse JMH benchmark data."""
    results_path = Path(results_dir)
    if not results_path.exists():
        print(f"Results directory {results_dir} not found")
        return pd.DataFrame()
    
    jmh_data = []
    jmh_files = list(results_path.glob("jmh-*.json"))
    
    for jmh_file in jmh_files:
        try:
            with open(jmh_file, 'r') as f:
                data = json.load(f)
            
            # Extract timestamp from filename (format: jmh-20250907T121845Z.json)
            timestamp_str = jmh_file.stem.replace('jmh-', '')
            timestamp = datetime.strptime(timestamp_str, '%Y%m%dT%H%M%SZ')
            
            # Process each benchmark result
            for result in data:
                benchmark_data = {
                    'timestamp': timestamp,
                    'benchmark': result['benchmark'],
                    'mode': result['mode'],
                    'score': result['primaryMetric']['score'],
                    'score_unit': result['primaryMetric']['scoreUnit'],
                    'score_error': result['primaryMetric'].get('scoreError', 'NaN'),
                    'params': result.get('params', {}),
                    'file': jmh_file.name
                }
                
                # Add parameter information
                if 'patternCount' in benchmark_data['params']:
                    benchmark_data['pattern_count'] = int(benchmark_data['params']['patternCount'])
                
                jmh_data.append(benchmark_data)
                
        except Exception as e:
            print(f"Error loading {jmh_file}: {e}")
            continue
    
    if jmh_data:
        df = pd.DataFrame(jmh_data)
        print(f"Loaded {len(df)} JMH benchmark results from {len(jmh_files)} files")
        return df
    else:
        print("No JMH data found")
        return pd.DataFrame()

def load_macro_data(results_dir="benchmarks/results"):
    """Load and parse macro benchmark data."""
    results_path = Path(results_dir)
    if not results_path.exists():
        print(f"Results directory {results_dir} not found")
        return pd.DataFrame()
    
    macro_data = []
    macro_files = list(results_path.glob("macro-*.json"))
    
    for macro_file in macro_files:
        try:
            with open(macro_file, 'r') as f:
                content = f.read()
                # Fix the problematic java field by replacing with a simpler version
                content = re.sub(
                    r'"java":\s*"[^"]*"[^"]*"[^"]*"[^"]*"[^"]*"[^"]*",\s*',
                    '"java_version": "openjdk-21",\n  ',
                    content
                )
                # Clean up any formatting issues
                content = re.sub(r',\s*,', ',', content)
                content = re.sub(r'{\s*,', '{', content)
                data = json.loads(content)
            
            # Extract timestamp from filename
            timestamp_str = macro_file.stem.replace('macro-', '')
            timestamp = datetime.strptime(timestamp_str, '%Y%m%dT%H%M%SZ')
            
            macro_record = {
                'timestamp': timestamp,
                'type': data.get('type', 'macro'),
                'duration_ms': data.get('duration_ms'),
                'exit_status': data.get('exit_status'),
                'max_regexps': data.get('args', {}).get('max_regexps'),
                'git_sha': data.get('git', {}).get('sha'),
                'git_branch': data.get('git', {}).get('branch'),
                'file': macro_file.name
            }
            
            macro_data.append(macro_record)
            
        except Exception as e:
            print(f"Error loading {macro_file}: {e}")
            continue
    
    if macro_data:
        df = pd.DataFrame(macro_data)
        print(f"Loaded {len(df)} macro benchmark results from {len(macro_files)} files")
        return df
    else:
        print("No macro data found")
        return pd.DataFrame()

def create_jmh_performance_chart(jmh_df, output_dir="charts"):
    """Create improved JMH performance evolution chart addressing issue requirements."""
    if jmh_df.empty:
        print("No JMH data available for charting")
        return
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Filter for the main benchmark
    main_benchmarks = jmh_df[jmh_df['benchmark'].str.contains('buildMatcher')]
    if main_benchmarks.empty:
        print("No buildMatcher benchmark data found")
        return
    
    # Sort by timestamp
    main_benchmarks = main_benchmarks.sort_values('timestamp')
    
    # Create figure with 2 panels (removed distribution chart as requested)
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12))
    fig.suptitle('JMH Benchmark Performance Evolution (Improved)', fontsize=16, fontweight='bold')
    
    # Plot 1: Performance over time focused on high pattern counts (5000, 10000)
    if 'pattern_count' in main_benchmarks.columns:
        # Focus on high pattern counts (5000, 10000) as specified in issue
        priority_pattern_counts = [5000, 10000]
        all_pattern_counts = sorted(main_benchmarks['pattern_count'].unique())
        
        # Use priority counts if available, otherwise show available data with warning
        pattern_counts_to_plot = [pc for pc in priority_pattern_counts if pc in all_pattern_counts]
        if not pattern_counts_to_plot:
            # If no high pattern counts available, show all available but prioritize highest
            pattern_counts_to_plot = sorted(all_pattern_counts, reverse=True)[:4]  # Show top 4 highest
            print(f"Warning: No 5000/10000 pattern data found. Showing highest available pattern counts: {pattern_counts_to_plot}")
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(pattern_counts_to_plot)))
        
        for i, pattern_count in enumerate(pattern_counts_to_plot):
            subset = main_benchmarks[main_benchmarks['pattern_count'] == pattern_count]
            if not subset.empty:
                ax1.plot(subset['timestamp'], subset['score'], 'o-', 
                        label=f'{pattern_count} patterns', 
                        color=colors[i], linewidth=2, markersize=6)
    else:
        ax1.plot(main_benchmarks['timestamp'], main_benchmarks['score'], 'o-', 
                color='#2E8B57', linewidth=2, markersize=6, label='Performance')
    
    ax1.set_title('Build Matcher Performance Over Time (Focus on 5000+ Patterns)', fontweight='bold')
    ax1.set_ylabel(f"Performance ({main_benchmarks['score_unit'].iloc[0]})")
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    # Include year in timestamp format as requested
    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))
    ax1.xaxis.set_major_locator(mdates.HourLocator(interval=6))
    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
    
    # Plot 2: Summary by pattern count (replaces distribution chart)
    if 'pattern_count' in main_benchmarks.columns and len(main_benchmarks) > 1:
        # Group by pattern count and calculate statistics
        pattern_stats = main_benchmarks.groupby('pattern_count')['score'].agg(['mean', 'std', 'count']).reset_index()
        pattern_stats = pattern_stats.sort_values('pattern_count')
        
        bars = ax2.bar(pattern_stats['pattern_count'], pattern_stats['mean'], 
                      yerr=pattern_stats['std'], alpha=0.7, capsize=5,
                      color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(pattern_stats)])
        
        ax2.set_title('Average Performance by Pattern Count (Replaces Distribution Chart)', fontweight='bold')
        ax2.set_xlabel('Pattern Count')
        ax2.set_ylabel(f"Average Performance ({main_benchmarks['score_unit'].iloc[0]})")
        ax2.grid(True, alpha=0.3)
        
        # Add sample size labels on bars
        for bar, count in zip(bars, pattern_stats['count']):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + (height*0.01 if height > 0 else 0.1),
                    f'n={count}', ha='center', va='bottom', fontsize=10)
    else:
        # Fallback for when pattern count data isn't available
        recent_data = main_benchmarks.tail(10)
        ax2.bar(range(len(recent_data)), recent_data['score'], color='#2E8B57', alpha=0.7)
        ax2.set_title('Recent Performance Results', fontweight='bold')
        ax2.set_ylabel(f"Performance ({main_benchmarks['score_unit'].iloc[0]})")
        ax2.set_xticks(range(len(recent_data)))
        ax2.set_xticklabels([t.strftime('%Y-%m-%d %H:%M') for t in recent_data['timestamp']], 
                          rotation=45, ha='right')
        ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, 'jmh_performance_evolution.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Improved JMH performance chart saved to {output_path}")

def create_comprehensive_comparison_chart(macro_df, output_dir="charts"):
    """Create comprehensive charts showing memory usage and rmatch vs Java comparisons."""
    if macro_df.empty:
        print("No macro data available for comprehensive comparison charting")
        return
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Filter for successful runs, prioritizing high pattern counts
    successful_runs = macro_df[macro_df['exit_status'] == 0]
    if successful_runs.empty:
        print("No successful macro benchmark runs found")
        return
    
    # Focus on high pattern counts (5000, 10000) as requested in issue
    high_pattern_runs = successful_runs[successful_runs['max_regexps'].isin([5000, 10000])]
    if high_pattern_runs.empty:
        high_pattern_runs = successful_runs  # Fallback to all successful runs
        print("Warning: No 5000/10000 pattern macro runs found, showing all available data")
    
    # Sort by timestamp
    high_pattern_runs = high_pattern_runs.sort_values('timestamp')
    
    # Create figure with multiple panels for the comprehensive view requested
    fig = plt.figure(figsize=(16, 14))
    gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1], hspace=0.3, wspace=0.3)
    
    # Panel 1: Execution time evolution for high pattern counts (main chart)
    ax1 = fig.add_subplot(gs[0, :])
    
    for pattern_count in sorted(high_pattern_runs['max_regexps'].unique()):
        subset = high_pattern_runs[high_pattern_runs['max_regexps'] == pattern_count]
        if not subset.empty:
            ax1.plot(subset['timestamp'], subset['duration_ms'], 'o-', 
                    label=f'{pattern_count} patterns', linewidth=2, markersize=6)
    
    ax1.set_title('Macro Test Execution Time Evolution (5000+ Patterns)', fontsize=14, fontweight='bold')
    ax1.set_ylabel('Execution Time (ms)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))
    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
    
    # Panel 2: Memory usage comparison (placeholder for structured data)
    ax2 = fig.add_subplot(gs[1, 0])
    ax2.text(0.5, 0.5, 'Memory Usage Over Time\n\n' + 
             'üìä Coming Soon:\n' +
             '‚Ä¢ rmatch memory usage trends\n' +
             '‚Ä¢ Java regex memory usage trends\n' +
             '‚Ä¢ Side-by-side comparison\n' +
             '‚Ä¢ For 5000 and 10000 pattern counts\n\n' +
             '‚öôÔ∏è Awaiting structured memory output\n' +
             'from ComprehensivePerformanceTest',
             ha='center', va='center', transform=ax2.transAxes,
             fontsize=11, bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
    ax2.set_title('Memory Usage Evolution (Pending)', fontweight='bold')
    ax2.axis('off')
    
    # Panel 3: Performance ratio metrics (placeholder for ratio data)
    ax3 = fig.add_subplot(gs[1, 1])
    ax3.text(0.5, 0.5, 'rmatch vs Java Ratios\n\n' +
             'üìà Performance Metrics:\n' +
             '‚Ä¢ Execution time ratio (rmatch/java)\n' +
             '‚Ä¢ Memory usage ratio (rmatch/java)\n' +
             '‚Ä¢ Trend analysis over time\n' +
             '‚Ä¢ Separate pane as requested\n\n' +
             '‚öôÔ∏è Ready for structured comparison data\n' +
             'from comprehensive benchmarks',
             ha='center', va='center', transform=ax3.transAxes,
             fontsize=11, bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgreen", alpha=0.8))
    ax3.set_title('Performance Ratio Metrics (Pending)', fontweight='bold')
    ax3.axis('off')
    
    # Panel 4: Pattern count performance summary
    ax4 = fig.add_subplot(gs[2, :])
    
    if len(high_pattern_runs) > 0:
        pattern_summary = high_pattern_runs.groupby('max_regexps')['duration_ms'].agg(['mean', 'std', 'count']).reset_index()
        
        if not pattern_summary.empty:
            bars = ax4.bar(pattern_summary['max_regexps'], pattern_summary['mean'], 
                          yerr=pattern_summary['std'], alpha=0.7, capsize=5, 
                          color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(pattern_summary)])
            ax4.set_title('Average Execution Time by Pattern Count (High Scale Focus)', fontweight='bold')
            ax4.set_xlabel('Pattern Count')
            ax4.set_ylabel('Average Execution Time (ms)')
            ax4.grid(True, alpha=0.3)
            
            # Add sample size labels on bars
            for bar, count in zip(bars, pattern_summary['count']):
                height = bar.get_height()
                if height > 0:
                    ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                            f'n={count}', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, 'comprehensive_performance_comparison.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Comprehensive performance comparison chart saved to {output_path}")

def create_combined_overview_chart(jmh_df, macro_df, output_dir="charts"):
    """Create a combined overview chart with key performance metrics."""
    os.makedirs(output_dir, exist_ok=True)
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('rmatch Performance Overview (Enhanced)', fontsize=18, fontweight='bold')
    
    # JMH Performance Summary
    if not jmh_df.empty:
        main_benchmarks = jmh_df[jmh_df['benchmark'].str.contains('buildMatcher')]
        if not main_benchmarks.empty:
            recent_jmh = main_benchmarks.sort_values('timestamp').tail(20)
            ax1.plot(recent_jmh['timestamp'], recent_jmh['score'], 'o-', 
                    color='#2E8B57', linewidth=2, markersize=4)
            ax1.set_title('JMH Build Performance')
            ax1.set_ylabel(f"Time ({recent_jmh['score_unit'].iloc[0]})")
            ax1.grid(True, alpha=0.3)
            # Include year in timestamp format
            ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
            plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
    
    # Macro Performance Summary (focus on high pattern counts)
    if not macro_df.empty:
        successful_macro = macro_df[macro_df['exit_status'] == 0].sort_values('timestamp').tail(20)
        if not successful_macro.empty:
            ax2.plot(successful_macro['timestamp'], successful_macro['duration_ms'], 'o-', 
                    color='#FF6B35', linewidth=2, markersize=4)
            ax2.set_title('Macro Benchmark Duration (High Scale)')
            ax2.set_ylabel('Duration (ms)')
            ax2.grid(True, alpha=0.3)
            ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
            plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)
    
    # Performance Statistics
    stats_data = []
    if not jmh_df.empty:
        main_benchmarks = jmh_df[jmh_df['benchmark'].str.contains('buildMatcher')]
        if not main_benchmarks.empty:
            recent_scores = main_benchmarks.sort_values('timestamp').tail(10)['score']
            stats_data.extend([
                f"JMH Avg: {recent_scores.mean():.2f} {main_benchmarks['score_unit'].iloc[0]}",
                f"JMH Std: {recent_scores.std():.2f} {main_benchmarks['score_unit'].iloc[0]}"
            ])
            
            # Add pattern count info
            if 'pattern_count' in main_benchmarks.columns:
                pattern_counts = sorted(main_benchmarks['pattern_count'].unique())
                stats_data.append(f"Pattern counts: {pattern_counts}")
    
    if not macro_df.empty:
        successful_macro = macro_df[macro_df['exit_status'] == 0]
        if not successful_macro.empty:
            recent_durations = successful_macro.sort_values('timestamp').tail(10)['duration_ms']
            stats_data.extend([
                f"Macro Avg: {recent_durations.mean():.0f} ms",
                f"Macro Std: {recent_durations.std():.0f} ms"
            ])
    
    # Display stats as text
    ax3.axis('off')
    ax3.text(0.1, 0.9, 'Recent Performance Statistics', fontsize=14, fontweight='bold', 
             transform=ax3.transAxes)
    for i, stat in enumerate(stats_data):
        ax3.text(0.1, 0.75 - i*0.1, stat, fontsize=11, transform=ax3.transAxes)
    
    # Data summary with issue improvements noted
    ax4.axis('off')
    summary_data = [
        f"JMH Benchmark Results: {len(jmh_df)}",
        f"Macro Benchmark Results: {len(macro_df)}"
    ]
    
    if not macro_df.empty and 'exit_status' in macro_df.columns:
        summary_data.append(f"Successful Macro Runs: {len(macro_df[macro_df['exit_status'] == 0])}")
    
    if not jmh_df.empty:
        date_range = jmh_df['timestamp'].max() - jmh_df['timestamp'].min()
        summary_data.append(f"Data Span: {date_range.days} days")
    
    # Add note about improvements
    summary_data.extend([
        "",
        "‚úì Timestamps include year",
        "‚úì Focus on 5000/10000 patterns", 
        "‚úì Distribution chart removed",
        "‚úì Memory & comparison ready"
    ])
    
    ax4.text(0.1, 0.9, 'Chart Improvements Summary', fontsize=14, fontweight='bold', 
             transform=ax4.transAxes)
    for i, summary in enumerate(summary_data):
        ax4.text(0.1, 0.8 - i*0.08, summary, fontsize=10, transform=ax4.transAxes)
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, 'performance_overview.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Enhanced performance overview chart saved to {output_path}")

def main():
    """Main function to generate all performance charts with issue improvements."""
    print("üöÄ Loading benchmark data for enhanced performance charts...")
    
    # Load data
    jmh_df = load_jmh_data()
    macro_df = load_macro_data()
    
    if jmh_df.empty and macro_df.empty:
        print("‚ùå No benchmark data found. Please run benchmarks first.")
        return
    
    print("üìä Generating enhanced performance charts...")
    
    # Create output directory
    os.makedirs("charts", exist_ok=True)
    
    # Generate improved charts
    if not jmh_df.empty:
        create_jmh_performance_chart(jmh_df)
    
    if not macro_df.empty:
        create_comprehensive_comparison_chart(macro_df)
    
    create_combined_overview_chart(jmh_df, macro_df)
    
    print("‚úÖ Enhanced performance chart generation complete!")
    print("\nüéØ GitHub Issue #161 Improvements Implemented:")
    print("  ‚úì Timestamps now include year (YYYY-MM-DD HH:MM format)")
    print("  ‚úì JMH chart focuses on 5000 and 10000 pattern counts (when available)")
    print("  ‚úì Removed 'performance distribution by pattern count' chart as requested")
    print("  ‚úì Added comprehensive comparison chart framework")
    print("  ‚úì Prepared placeholders for memory usage and rmatch vs Java ratio metrics")
    print("  ‚úì Enhanced chart titles and formatting")
    print("\nüìù Next Steps:")
    print("  ‚Ä¢ Run benchmarks with 5000 and 10000 patterns to populate high-scale data")
    print("  ‚Ä¢ Implement structured output from ComprehensivePerformanceTest for memory/ratio data")
    print("  ‚Ä¢ Charts will automatically improve as more comprehensive data becomes available")

if __name__ == "__main__":
    main()